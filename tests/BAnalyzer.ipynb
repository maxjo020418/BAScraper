{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "from os import path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = '../data'\n",
    "data_path = path.join(cwd, '01.json')\n",
    "student_path = path.join(cwd, 'students.txt')\n",
    "\n",
    "exclude_author = ['AutoModerator', 'BlueArchive-ModTeam']\n",
    "\n",
    "with open(data_path, \"r\", encoding='utf-8') as outfile:\n",
    "    data = json.load(outfile)\n",
    "\n",
    "data_df = pandas.DataFrame.from_dict(data, orient='index')\n",
    "# [print(col) for col in data_df.columns]\n",
    "filtered_df = data_df[['title', 'link_flair_text', 'ups', 'created_utc', 'comments', 'num_comments']]\n",
    "filtered_df.created_utc = filtered_df.created_utc.apply(lambda x: datetime.fromtimestamp(x))\n",
    "filtered_df.comments = filtered_df.comments.apply(lambda x: [comment['body'] for comment in x if comment['author'] not in exclude_author])\n",
    "filtered_df.num_comments = filtered_df.comments.apply(lambda x: len(x))\n",
    "filtered_df = filtered_df.loc[filtered_df['link_flair_text'].isin(['OC ART ', 'NON OC ART ', 'Comic/TL'])]\n",
    "\n",
    "display(filtered_df.nlargest(10, 'num_comments'))\n",
    "Counter(filtered_df.link_flair_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "patterns = [\n",
    "    r\"!\\[img\\]\\(emote\\|t5_[a-z0-9]+\\|\\d+\\)\", # subreddit sticker/emote regex\n",
    "    r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", # http(s) link regex\n",
    "]\n",
    "\n",
    "def clean(text):\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    return html.unescape(text).lower()\n",
    "\n",
    "data_text = { i: [v.title] + v.comments for i, v in filtered_df.iterrows()}\n",
    "data_text = { i: clean('\\n'.join(post_group)) for i, post_group in data_text.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy model structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacymoji import Emoji\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(spacy.require_gpu())\n",
    "\n",
    "ba_character_list = list()\n",
    "with open(student_path, 'r', encoding='utf-8') as f:\n",
    "    for l in f.readlines():\n",
    "        ba_character_list.append(l.strip())\n",
    "\n",
    "# alias groups - (main name, alias-1, alias-2, alias-3, ...)\n",
    "alias_groups = [\n",
    "    ('aris', 'alice', 'arisu', 'fridge', 'refrigerator'),\n",
    "    ('hoshino', 'oji-san', 'ojisan', 'oji san'),\n",
    "    ('yuuka', 'yuka', 'calculator', '100kg', '100 kg'),\n",
    "    # ('sensei', 'senseis'), # cannot be detected as plural\n",
    "    ('hina', 'hinature', 'hiniature')\n",
    "]\n",
    "\n",
    "ba_character_list.extend([j for i in alias_groups for j in i])\n",
    "\n",
    "print('ba_character_list:', len(ba_character_list))\n",
    "print(ba_character_list)\n",
    "\n",
    "\n",
    "def BA_char_finder(tk):\n",
    "    if (student := tk.text.lower()) in ba_character_list:\n",
    "        for group in alias_groups:\n",
    "            if student in group:\n",
    "                student = group[0]\n",
    "        return student\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_nbors_(tk: spacy.tokens.token.Token, step: int, N: int):\n",
    "    # step usually should be 1 or -1 - searching range step\n",
    "    # N is the Nth search result to be returned\n",
    "\n",
    "    if tk.is_sent_start and step < 0:\n",
    "        return\n",
    "    elif tk.is_sent_end and step > 0:\n",
    "        return\n",
    "    \n",
    "    n = 1\n",
    "    i = step\n",
    "    while True:\n",
    "        nbor = tk.nbor(i)\n",
    "        if nbor.is_stop or nbor._.is_emoji or not nbor.is_alpha:\n",
    "            pass\n",
    "        else:\n",
    "            if n == N:\n",
    "                return nbor\n",
    "            else:\n",
    "                n += 1\n",
    "                pass\n",
    "        \n",
    "        if nbor.is_sent_end or nbor.is_sent_start:\n",
    "            return\n",
    "\n",
    "        i += step\n",
    "\n",
    "\n",
    "def get_nbors(tk):\n",
    "    return ((\n",
    "        (\n",
    "        get_nbors_(tk, -1, 1), # l1\n",
    "        get_nbors_(tk, 1, 1), # r1\n",
    "        ),\n",
    "        (\n",
    "        get_nbors_(tk, -1, 2), # l2\n",
    "        get_nbors_(tk, 1, 2), # r2\n",
    "        ),\n",
    "        # tk.sent\n",
    "    ))\n",
    "\n",
    "# for splitting sentences\n",
    "@spacy.language.Language.component(\"custom_boundary\")\n",
    "def custom_boundary(doc):\n",
    "\n",
    "    delimiters=['...', '\\n']\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        if token.text in delimiters:\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.add_pipe('custom_boundary', before='parser')\n",
    "\n",
    "Token.set_extension(\"nbors\", getter=get_nbors, force=True)\n",
    "Token.set_extension(\"ba_characters\", getter=BA_char_finder, force=True)\n",
    "\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "\n",
    "print(spacy.require_gpu())\n",
    "\"\"\"\n",
    "Cuda compilation tools, release 11.5, V11.5.119\n",
    "Build cuda_11.5.r11.5/compiler.30672275_0\n",
    "\"\"\"\n",
    "\n",
    "docs = list()\n",
    "for t in tqdm(data_text.values()):\n",
    "    docs.append(nlp(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText training\n",
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cleanable(tk: spacy.tokens.token.Token):\n",
    "    # might(?) try and add emoji if that seems plausible\n",
    "    if (tk.is_alpha and not tk.is_stop and not tk._.is_emoji) or tk.is_sent_end:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocess_tk(tk: spacy.tokens.token.Token):\n",
    "    if not tk.is_alpha:\n",
    "        tk = '\\n'\n",
    "    elif tk.is_sent_end:\n",
    "        tk = tk.lemma_.lower().strip()\n",
    "        for group in alias_groups:\n",
    "            if tk in group:\n",
    "                tk = group[0]\n",
    "        tk += '\\n'\n",
    "    else:\n",
    "        tk = tk.lemma_.lower().strip()\n",
    "        for group in alias_groups:\n",
    "            if tk in group:\n",
    "                tk = group[0]\n",
    "        tk += ' '\n",
    "    \n",
    "    return tk\n",
    "\n",
    "\n",
    "# preprocessed text for FastText\n",
    "ft_preprocessed = [preprocess_tk(tk) for doc in docs for tk in doc if is_cleanable(tk)]\n",
    "print('no. of tokens:', len(ft_preprocessed))\n",
    "ft_preprocessed = ''.join(ft_preprocessed)\n",
    "ft_preprocessed = re.sub(\" +\", \" \", ft_preprocessed) # fixes double(or more) spaces\n",
    "ft_preprocessed = re.sub(\"<[^>]*>\", \"\", ft_preprocessed) # remove html tags\n",
    "\n",
    "print('no. of comments:', ft_preprocessed.count('\\n'))\n",
    "\n",
    "ft_path = path.join(cwd, 'ft_preprocessed.txt')\n",
    "with open(ft_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(ft_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "print(ft_path)\n",
    "model = fasttext.train_unsupervised(ft_path, dim=300)  # pretrainedVectors=path.join(cwd, 'crawl-300d-2M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save vector as CSV (for tensorflow projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "labels = model.get_labels()\n",
    "\n",
    "with open(path.join(cwd, 'vectors.tsv'), 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for label in labels:\n",
    "        writer.writerow(list(model.get_word_vector(label)))\n",
    "\n",
    "with open(path.join(cwd, 'labels.tsv'), 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for label in labels:\n",
    "        writer.writerow([label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Inject the vocab vector table & re-run the pipeline~~ don't need this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.reset_vectors(width=model.get_dimension())\n",
    "[nlp.vocab.set_vector(word, model.get_word_vector(word)) for word in model.get_labels()]\n",
    "nlp.vocab.deduplicate_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list()\n",
    "for t in tqdm(data_text.values()):\n",
    "    docs.append(nlp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([tk._.ba_characters for doc in docs for tk in doc if tk._.ba_characters]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get vec similarity for nbor words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "# will be the target ba_caharcter to calculate the similarity/score\n",
    "target_character = 'hina'\n",
    "\n",
    "# `cm` as in correlation matrix (not exactly a matrix but...)\n",
    "cm_original = [tk._.nbors for doc in tqdm(docs) for tk in doc if tk._.ba_characters == target_character]\n",
    "\n",
    "cm1_original = [j for i in (cm_original) for j in i[0] if j and (j._.ba_characters != target_character)] # bigram range\n",
    "cm2_original = [j for i in (cm_original) for j in i[1] if j and (j._.ba_characters != target_character)] # trigram range\n",
    "\n",
    "print('cm_1', cm1 := Counter([i.text for i in cm1_original]).most_common(500))\n",
    "print('cm_2', cm2 := Counter([i.text for i in cm2_original]).most_common(500))\n",
    "\n",
    "cm1_words = [i[0] for i in cm1]\n",
    "cm2_words = [i[0] for i in cm2]\n",
    "\n",
    "cm1_count = np.array([i[1] for i in cm1])\n",
    "cm2_count = np.array([i[1] for i in cm2])\n",
    "\n",
    "cm1_normalized_count = {k: v for k, v in zip(cm1_words, preprocessing.normalize([cm1_count])[0])}\n",
    "cm2_normalized_count = {k: v for k, v in zip(cm2_words, preprocessing.normalize([cm2_count])[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make it so that it'll compare the raw vector values. - use cosine similarity\n",
    "from scipy import spatial\n",
    "\n",
    "def sort_dict(inp: dict) -> dict:\n",
    "    return dict(sorted(inp.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def get_vec_similarity(word1, word2):\n",
    "    return 1 - spatial.distance.cosine(model.get_word_vector(word1), model.get_word_vector(word2))\\\n",
    "\n",
    "def penalty(k: str, v): \n",
    "    # give a penalty to the score of students since students would naturally have higher similarity scores\n",
    "    return v/2 if k in ba_character_list + ['chan', 'probably'] else v\n",
    "\n",
    "cm1_vec = {word: get_vec_similarity(word, target_character) for word in cm1_words}\n",
    "cm1_vec = {k: (v + cm1_normalized_count[k] * 10) for k, v in cm1_vec.items()}\n",
    "cm1_vec = sort_dict(cm1_vec)\n",
    "# display(cm1_vec)\n",
    "\n",
    "cm2_vec = {word: get_vec_similarity(word, target_character) for word in cm2_words}\n",
    "cm2_vec = {k: (v + cm2_normalized_count[k] * 10)/4 for k, v in cm2_vec.items()}\n",
    "cm2_vec = sort_dict(cm2_vec)\n",
    "# display(cm2_vec)\n",
    "\n",
    "score = {k: penalty(k, v + cm2_vec.get(k, 0)) for k, v in cm1_vec.items()} # add up cm1 and cm2\n",
    "score = sort_dict(score)\n",
    "\n",
    "top_n = 10\n",
    "for k in score:\n",
    "    print(f'{k}: {score[k]}')\n",
    "    top_n -= 1\n",
    "    if top_n == 0: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "mask = np.array(Image.open(path.join(cwd, 'hina.png')))\n",
    "\n",
    "wordcloud = WordCloud(random_state=1,\n",
    "                      width = 2000, height = 2000,\n",
    "                      background_color='black',\n",
    "                      mask = mask,\n",
    "                      max_words = 1000\n",
    "                      ).generate_from_frequencies(score)\n",
    "\n",
    "# Plot\n",
    "image_colors = ImageColorGenerator(mask)\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\") # wordcloud.recolor(color_func = lambda *args, **kwargs: \"black\") for single color\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(path.join(cwd, 'wordcloud.png'), dpi=2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BAScraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
