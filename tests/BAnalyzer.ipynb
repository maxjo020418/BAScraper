{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - started thread no.0 | 2024-01-01 06:00:00 <- 2024-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no CSE credentials detected. you'll need it to use the Google Custom Search Engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - started thread no.1 | 2024-01-01 12:00:00 <- 2024-01-01 06:00:00\n",
      "INFO - started thread no.2 | 2024-01-01 18:00:00 <- 2024-01-01 12:00:00\n",
      "INFO - started thread no.3 | 2024-01-02 00:00:00 <- 2024-01-01 18:00:00\n",
      "INFO - t-1: 200 - 0:00:00.942073\n",
      "INFO - t-2: 200 - 0:00:00.932266\n",
      "INFO - t-0: 200 - 0:00:01.257051\n",
      "INFO - t-3: 200 - 0:00:00.957824\n",
      "INFO - t-1: 200 - 0:00:01.105028\n",
      "INFO - t-2: 200 - 0:00:00.568458\n",
      "INFO - t-0: 200 - 0:00:00.432588\n",
      "INFO - t-3: 200 - 0:00:00.865668\n",
      "INFO - t-1: finished.\n",
      "INFO - t-2: finished.\n",
      "INFO - t-0: finished.\n",
      "INFO - t-3: finished.\n",
      "INFO - submission fetching time: 12.53048825263977sec\n",
      "INFO - dupe detected for 18vwza2\n",
      "INFO - dupe detected for 18vwws2\n",
      "INFO - dupe detected for 18vszv8\n",
      "INFO - dupe detected for 18vs7jk\n",
      "INFO - dupe detected for 18vqn02\n",
      "INFO - dupe detected for 18vqb9b\n",
      "INFO - dupe detected for 18vqaq8\n",
      "INFO - dupe detected for 18vo28d\n",
      "INFO - dupe detected for 18vllwi\n",
      "INFO - dupe detected for 18vll0o\n",
      "INFO - dupe detected for 18vlkms\n",
      "INFO - dupe detected for 18vkmki\n",
      "INFO - dupe detected for 18vej0w\n",
      "INFO - dupe detected for 18vdqv4\n",
      "INFO - dupe detected for 18vcrd7\n",
      "INFO - dupe detected for 18vcp8v\n",
      "INFO - dupe detected for 18vb5r2\n",
      "INFO - starting comment fetching...\n",
      "INFO - t-0: 122 comments groups left\n",
      "INFO - t-1: 121 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.930816\n",
      "INFO - t-2: 120 comments groups left\n",
      "INFO - t-3: 119 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.912427\n",
      "INFO - t-2: 200 - 0:00:00.924731\n",
      "INFO - t-3: 200 - 0:00:00.977040\n",
      "INFO - t-0: 118 comments groups left\n",
      "INFO - t-1: 117 comments groups left\n",
      "INFO - t-2: 116 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.941148\n",
      "INFO - t-3: 115 comments groups left\n",
      "INFO - t-1: 200 - 0:00:01.149554\n",
      "INFO - t-3: 200 - 0:00:01.422463\n",
      "INFO - t-2: 200 - 0:00:02.097238\n",
      "INFO - t-0: 114 comments groups left\n",
      "INFO - t-1: 113 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.938356\n",
      "INFO - t-1: 200 - 0:00:00.907262\n",
      "INFO - t-3: 112 comments groups left\n",
      "INFO - t-2: 111 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.948222\n",
      "INFO - t-2: 200 - 0:00:01.223302\n",
      "INFO - t-0: 110 comments groups left\n",
      "INFO - t-1: 109 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.560571\n",
      "INFO - t-1: 200 - 0:00:00.913716\n",
      "INFO - t-3: 108 comments groups left\n",
      "INFO - t-2: 107 comments groups left\n",
      "INFO - t-3: 200 - 0:00:01.126260\n",
      "INFO - t-0: 106 comments groups left\n",
      "INFO - t-2: 200 - 0:00:01.447611\n",
      "INFO - t-1: 105 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.481051\n",
      "INFO - t-0: 200 - 0:00:01.298060\n",
      "INFO - t-3: 104 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.543669\n",
      "INFO - t-2: 103 comments groups left\n",
      "INFO - t-2: 200 - 0:00:01.232836\n",
      "INFO - t-1: 102 comments groups left\n",
      "INFO - t-0: 101 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.909701\n",
      "INFO - t-3: 100 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.928174\n",
      "INFO - t-3: 200 - 0:00:00.934407\n",
      "INFO - t-2: 99 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.918466\n",
      "INFO - t-1: 98 comments groups left\n",
      "INFO - t-0: 97 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.469614\n",
      "INFO - t-3: 96 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.971540\n",
      "INFO - t-3: 200 - 0:00:00.925596\n",
      "INFO - t-2: 95 comments groups left\n",
      "INFO - t-1: 94 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.940628\n",
      "INFO - t-1: 200 - 0:00:00.903225\n",
      "INFO - t-0: 93 comments groups left\n",
      "INFO - t-3: 92 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.933197\n",
      "INFO - t-2: 91 comments groups left\n",
      "INFO - t-1: 90 comments groups left\n",
      "INFO - t-3: 200 - 0:00:02.096312\n",
      "INFO - t-1: 200 - 0:00:00.462238\n",
      "INFO - t-0: 89 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.572213\n",
      "INFO - t-3: 88 comments groups left\n",
      "INFO - t-1: 87 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.415746\n",
      "INFO - t-3: 200 - 0:00:01.175429\n",
      "INFO - t-2: 86 comments groups left\n",
      "INFO - t-0: 200 - 0:00:01.805407\n",
      "INFO - t-2: 200 - 0:00:01.208980\n",
      "INFO - t-1: 85 comments groups left\n",
      "INFO - t-3: 84 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.555684\n",
      "INFO - t-3: 200 - 0:00:00.906630\n",
      "INFO - t-0: 83 comments groups left\n",
      "INFO - t-2: 82 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.951302\n",
      "INFO - t-2: 200 - 0:00:00.939959\n",
      "INFO - t-1: 81 comments groups left\n",
      "INFO - t-3: 80 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.867123\n",
      "INFO - t-3: 200 - 0:00:00.936678\n",
      "INFO - t-0: 79 comments groups left\n",
      "INFO - t-2: 78 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.937566\n",
      "INFO - t-1: 77 comments groups left\n",
      "INFO - t-3: 76 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.912899\n",
      "INFO - t-3: 200 - 0:00:00.923717\n",
      "INFO - t-0: 75 comments groups left\n",
      "INFO - t-2: 200 - 0:00:02.813365\n",
      "INFO - t-1: 74 comments groups left\n",
      "INFO - t-3: 73 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.477737\n",
      "INFO - t-2: 72 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.917107\n",
      "INFO - t-1: 71 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.905678\n",
      "INFO - t-2: 70 comments groups left\n",
      "INFO - t-0: 200 - 0:00:07.843617\n",
      "INFO - t-2: 200 - 0:00:00.488100\n",
      "INFO - t-1: 69 comments groups left\n",
      "INFO - t-0: 68 comments groups left\n",
      "INFO - t-1: 200 - 0:00:01.221416\n",
      "INFO - t-2: 67 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.481400\n",
      "INFO - t-0: 200 - 0:00:00.932787\n",
      "INFO - t-3: 200 - 0:00:06.647884\n",
      "INFO - t-1: 66 comments groups left\n",
      "INFO - t-2: 65 comments groups left\n",
      "INFO - t-0: 64 comments groups left\n",
      "INFO - t-2: 200 - 0:00:01.128415\n",
      "INFO - t-3: 63 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.933338\n",
      "INFO - t-3: 200 - 0:00:00.900445\n",
      "INFO - t-1: 200 - 0:00:00.616875\n",
      "INFO - t-2: 62 comments groups left\n",
      "INFO - t-0: 61 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.928594\n",
      "INFO - t-3: 60 comments groups left\n",
      "INFO - t-0: 200 - 0:00:01.148835\n",
      "INFO - t-1: 59 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.945466\n",
      "INFO - t-1: 200 - 0:00:01.250758\n",
      "INFO - t-2: 58 comments groups left\n",
      "INFO - t-0: 57 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.556923\n",
      "INFO - t-3: 56 comments groups left\n",
      "INFO - t-1: 55 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.923316\n",
      "INFO - t-0: 200 - 0:00:01.052296\n",
      "INFO - t-1: 200 - 0:00:01.011207\n",
      "INFO - t-2: 54 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.493435\n",
      "INFO - t-3: 53 comments groups left\n",
      "INFO - t-0: 52 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.495238\n",
      "INFO - t-0: 200 - 0:00:00.920523\n",
      "INFO - t-1: 51 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.568352\n",
      "INFO - t-2: 50 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.919063\n",
      "INFO - t-3: 49 comments groups left\n",
      "INFO - t-0: 48 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.457484\n",
      "INFO - t-1: 47 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.477390\n",
      "INFO - t-0: 200 - 0:00:01.225472\n",
      "INFO - t-2: 46 comments groups left\n",
      "INFO - t-3: 45 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.925887\n",
      "INFO - t-3: 200 - 0:00:00.926761\n",
      "INFO - t-1: 44 comments groups left\n",
      "INFO - t-0: 43 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.944170\n",
      "INFO - t-2: 42 comments groups left\n",
      "INFO - t-3: 41 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.487536\n",
      "INFO - t-1: 200 - 0:00:01.212084\n",
      "INFO - t-3: 200 - 0:00:00.950017\n",
      "INFO - t-0: 40 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.576073\n",
      "INFO - t-2: 39 comments groups left\n",
      "INFO - t-1: 38 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.911661\n",
      "INFO - t-2: 200 - 0:00:00.567065\n",
      "INFO - t-3: 37 comments groups left\n",
      "INFO - t-0: 36 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.927465\n",
      "INFO - t-0: 200 - 0:00:00.909666\n",
      "INFO - t-1: 35 comments groups left\n",
      "INFO - t-2: 34 comments groups left\n",
      "INFO - t-3: 33 comments groups left\n",
      "INFO - t-1: 200 - 0:00:01.154247\n",
      "INFO - t-0: 32 comments groups left\n",
      "INFO - t-2: 200 - 0:00:01.886590\n",
      "INFO - t-3: 200 - 0:00:00.930090\n",
      "INFO - t-0: 200 - 0:00:01.041144\n",
      "INFO - t-1: 31 comments groups left\n",
      "INFO - t-2: 30 comments groups left\n",
      "INFO - t-3: 29 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.917060\n",
      "INFO - t-0: 28 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.918460\n",
      "INFO - t-0: 200 - 0:00:00.966850\n",
      "INFO - t-2: 200 - 0:00:01.963696\n",
      "INFO - t-1: 27 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.476058\n",
      "INFO - t-3: 26 comments groups left\n",
      "INFO - t-0: 25 comments groups left\n",
      "INFO - t-2: 25 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.921818\n",
      "INFO - t-2: 200 - 0:00:00.690606\n",
      "INFO - t-0: 200 - 0:00:01.084536\n",
      "INFO - t-1: 23 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.877610\n",
      "INFO - t-3: 22 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.496641\n",
      "INFO - t-2: 21 comments groups left\n",
      "INFO - t-0: 20 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.912503\n",
      "INFO - t-0: 200 - 0:00:00.932555\n",
      "INFO - t-1: 19 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.477387\n",
      "INFO - t-3: 18 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.939118\n",
      "INFO - t-2: 17 comments groups left\n",
      "INFO - t-0: 16 comments groups left\n",
      "INFO - t-1: 15 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.915216\n",
      "INFO - t-2: 200 - 0:00:01.379636\n",
      "INFO - t-1: 200 - 0:00:00.903539\n",
      "INFO - t-3: 14 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.924815\n",
      "INFO - t-0: 13 comments groups left\n",
      "INFO - t-2: 12 comments groups left\n",
      "INFO - t-1: 11 comments groups left\n",
      "INFO - t-0: 200 - 0:00:01.113459\n",
      "INFO - t-2: 200 - 0:00:01.130230\n",
      "INFO - t-1: 200 - 0:00:01.208853\n",
      "INFO - t-3: 10 comments groups left\n",
      "INFO - t-3: 200 - 0:00:01.358263\n",
      "INFO - t-0: 9 comments groups left\n",
      "INFO - t-2: 8 comments groups left\n",
      "INFO - t-0: 200 - 0:00:00.475468\n",
      "INFO - t-2: 200 - 0:00:00.473963\n",
      "INFO - t-1: 7 comments groups left\n",
      "INFO - t-1: 200 - 0:00:00.581074\n",
      "INFO - t-3: 6 comments groups left\n",
      "INFO - t-0: 5 comments groups left\n",
      "INFO - t-2: 4 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.912884\n",
      "INFO - t-0: 200 - 0:00:00.924699\n",
      "INFO - t-1: 3 comments groups left\n",
      "INFO - t-2: 200 - 0:00:00.912661\n",
      "INFO - t-1: 200 - 0:00:00.929328\n",
      "INFO - t-3: 2 comments groups left\n",
      "INFO - t-3: 200 - 0:00:00.477795\n",
      "INFO - t-0: 1 comments groups left\n",
      "INFO - dupe detected for kftuk04\n",
      "INFO - dupe detected for kfsrztg\n",
      "INFO - dupe detected for kfsj35c\n",
      "INFO - dupe detected for kfrlsby\n",
      "INFO - dupe detected for kfscpwl\n",
      "INFO - dupe detected for kfqlt9o\n",
      "INFO - t-0: 200 - 0:00:00.929267\n",
      "INFO - dupe detected for kfu1vci\n",
      "INFO - dupe detected for kft68tz\n",
      "INFO - dupe detected for kfstluo\n",
      "INFO - dupe detected for kfsc13j\n",
      "INFO - dupe detected for kfthu5b\n",
      "INFO - dupe detected for kfr5p2c\n",
      "INFO - dupe detected for kftsrut\n",
      "INFO - dupe detected for kfs164y\n",
      "INFO - dupe detected for kfq5g50\n",
      "INFO - dupe detected for kfukm6r\n",
      "INFO - dupe detected for kfu6s3p\n",
      "INFO - dupe detected for kftrl6w\n",
      "INFO - dupe detected for kfstjzh\n",
      "INFO - dupe detected for kfsseu3\n",
      "INFO - dupe detected for kftf8m2\n",
      "INFO - dupe detected for kfsq5c4\n",
      "INFO - dupe detected for kfrxpq6\n",
      "INFO - dupe detected for kfqecsa\n",
      "INFO - comment fetching time: 135.80856776237488sec\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from BAScraper import Pushpull\n",
    "from os import path\n",
    "\n",
    "cwd = '../results'\n",
    "data_path = path.join(cwd, 'data.json')\n",
    "\n",
    "pp = Pushpull(sleepsec=3, threads=4, cwd=cwd)\n",
    "\n",
    "data = pp.get_submissions(after=datetime(2024, 1, 1), before=datetime(2024, 1, 2),\n",
    "                          subreddit='bluearchive', get_comments=True, duplicate_action='keep_original')\n",
    "\n",
    "with open(data_path, \"w\", encoding='utf-8') as outfile:\n",
    "    json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_889/3429226781.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n",
      "/tmp/ipykernel_889/3429226781.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.created_utc = filtered_df.created_utc.apply(lambda x: datetime.fromtimestamp(x))\n",
      "/tmp/ipykernel_889/3429226781.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.comments = filtered_df.comments.apply(lambda x: [comment['body'] for comment in x if comment['author'] not in exclude_author])\n",
      "/tmp/ipykernel_889/3429226781.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.num_comments = filtered_df.comments.apply(lambda x: len(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>ups</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>comments</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18vokb1</th>\n",
       "      <td>Hoshino's Intrusive Thoughts Leaked (Translate...</td>\n",
       "      <td>Comic/TL</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 13:03:14</td>\n",
       "      <td>[Don't send help I'm perfectly happy here ![im...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vju8z</th>\n",
       "      <td>I'm going insane again (Mika, Tsurugi, Hina)</td>\n",
       "      <td>OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 08:25:46</td>\n",
       "      <td>[fire.., Hina scars 🥵, For real, I don't get i...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vnj59</th>\n",
       "      <td>New Year Mutsuki (by @12beeeef)</td>\n",
       "      <td>NON OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 11:57:06</td>\n",
       "      <td>[nah, nai v3 can already do halos in different...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vdmi2</th>\n",
       "      <td>New Year's Blessing be upon you! [Plana, Arona...</td>\n",
       "      <td>Comic/TL</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 03:19:10</td>\n",
       "      <td>[I'll take your feet plana-chan, Don’t even jo...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vw375</th>\n",
       "      <td>\"Punishment Time!\" - Nonomi versus heavy armor...</td>\n",
       "      <td>NON OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 21:47:06</td>\n",
       "      <td>[The others wouldn't let her, It's been so lon...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vo28d</th>\n",
       "      <td>I drew Shizuko :3</td>\n",
       "      <td>OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 12:30:36</td>\n",
       "      <td>[Noooooo😭😭😭, Nooooooo, 🔥🔥🔥✍️✍️✍️🔥🔥🔥, Hot, To P...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vvs4q</th>\n",
       "      <td>Swimsuit Valkyrie mob-chan in a bind (by @HuDe...</td>\n",
       "      <td>NON OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 21:26:15</td>\n",
       "      <td>[So true brother, honestly i like the more ado...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vu04w</th>\n",
       "      <td>Miyu and... Mr Seagull? (By @ylgcoffee)</td>\n",
       "      <td>NON OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 19:20:56</td>\n",
       "      <td>[Mr Barry Chopsticks the seagull, Sir, don't d...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18voxc0</th>\n",
       "      <td>Happy New Year, fellow Sensei!!! Ready for ano...</td>\n",
       "      <td>OC ART</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 13:26:50</td>\n",
       "      <td>[Worst pulls? S. Hoshino banner. = sparked\\n\\n...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18vn9lc</th>\n",
       "      <td>New Year Pocket Money [Himari, Momoi, Midori] ...</td>\n",
       "      <td>Comic/TL</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 11:40:30</td>\n",
       "      <td>[Wow, rude, First and last time that Sensei al...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title link_flair_text  \\\n",
       "18vokb1  Hoshino's Intrusive Thoughts Leaked (Translate...        Comic/TL   \n",
       "18vju8z       I'm going insane again (Mika, Tsurugi, Hina)         OC ART    \n",
       "18vnj59                    New Year Mutsuki (by @12beeeef)     NON OC ART    \n",
       "18vdmi2  New Year's Blessing be upon you! [Plana, Arona...        Comic/TL   \n",
       "18vw375  \"Punishment Time!\" - Nonomi versus heavy armor...     NON OC ART    \n",
       "18vo28d                                  I drew Shizuko :3         OC ART    \n",
       "18vvs4q  Swimsuit Valkyrie mob-chan in a bind (by @HuDe...     NON OC ART    \n",
       "18vu04w            Miyu and... Mr Seagull? (By @ylgcoffee)     NON OC ART    \n",
       "18voxc0  Happy New Year, fellow Sensei!!! Ready for ano...         OC ART    \n",
       "18vn9lc  New Year Pocket Money [Himari, Momoi, Midori] ...        Comic/TL   \n",
       "\n",
       "         ups         created_utc  \\\n",
       "18vokb1    1 2024-01-01 13:03:14   \n",
       "18vju8z    1 2024-01-01 08:25:46   \n",
       "18vnj59    1 2024-01-01 11:57:06   \n",
       "18vdmi2    1 2024-01-01 03:19:10   \n",
       "18vw375    1 2024-01-01 21:47:06   \n",
       "18vo28d    1 2024-01-01 12:30:36   \n",
       "18vvs4q    1 2024-01-01 21:26:15   \n",
       "18vu04w    1 2024-01-01 19:20:56   \n",
       "18voxc0    1 2024-01-01 13:26:50   \n",
       "18vn9lc    1 2024-01-01 11:40:30   \n",
       "\n",
       "                                                  comments  num_comments  \n",
       "18vokb1  [Don't send help I'm perfectly happy here ![im...            45  \n",
       "18vju8z  [fire.., Hina scars 🥵, For real, I don't get i...            44  \n",
       "18vnj59  [nah, nai v3 can already do halos in different...            41  \n",
       "18vdmi2  [I'll take your feet plana-chan, Don’t even jo...            39  \n",
       "18vw375  [The others wouldn't let her, It's been so lon...            34  \n",
       "18vo28d  [Noooooo😭😭😭, Nooooooo, 🔥🔥🔥✍️✍️✍️🔥🔥🔥, Hot, To P...            30  \n",
       "18vvs4q  [So true brother, honestly i like the more ado...            29  \n",
       "18vu04w  [Mr Barry Chopsticks the seagull, Sir, don't d...            28  \n",
       "18voxc0  [Worst pulls? S. Hoshino banner. = sparked\\n\\n...            28  \n",
       "18vn9lc  [Wow, rude, First and last time that Sensei al...            25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'NON OC ART ': 38, 'OC ART ': 26, 'Comic/TL': 8})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "from os import path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = '../results'\n",
    "data_path = path.join(cwd, 'data.json')\n",
    "student_path = path.join(cwd, 'students.txt')\n",
    "\n",
    "exclude_author = ['AutoModerator', 'BlueArchive-ModTeam']\n",
    "\n",
    "with open(data_path, \"r\", encoding='utf-8') as outfile:\n",
    "    data = json.load(outfile)\n",
    "\n",
    "data_df = pandas.DataFrame.from_dict(data, orient='index')\n",
    "# [print(col) for col in data_df.columns]\n",
    "filtered_df = data_df[['title', 'link_flair_text', 'ups', 'created_utc', 'comments', 'num_comments']]\n",
    "filtered_df.created_utc = filtered_df.created_utc.apply(lambda x: datetime.fromtimestamp(x))\n",
    "filtered_df.comments = filtered_df.comments.apply(lambda x: [comment['body'] for comment in x if comment['author'] not in exclude_author])\n",
    "filtered_df.num_comments = filtered_df.comments.apply(lambda x: len(x))\n",
    "filtered_df = filtered_df.loc[filtered_df['link_flair_text'].isin(['OC ART ', 'NON OC ART ', 'Comic/TL'])]\n",
    "\n",
    "display(filtered_df.nlargest(10, 'num_comments'))\n",
    "Counter(filtered_df.link_flair_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the title for the post is appended to the start of the comment doc group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hoshino\\'s Intrusive Thoughts Leaked (Translated) [kuuload]\\nDon\\'t send help I\\'m perfectly happy here \\n\\nMom\\'s spaghetti\\nThat sounds like some Slaneeshi demon to me.\\nPoor sleepy oji-san\\nBased\\nSensei cant leave if he cant walk,  \\n\\nProceeds to blow of senseis leg with her shotgun\\nI won\\'t go anywhere without my beloved uncle. \\nI have no idea what that is and I have a feeling I don\\'t wanna know \\n r/japanpeopletwitter moment.\\nHoshino I\\'m gonna be ok. You can go rest now.\\nMom sketty vomit sweater ready *n e r v o u s*\\nIn this state, Hoshino\\'s ahoge is the only part of her going atsui.\\nFastest UOOGH in Kivotos\\nDuring an official comic she immediately masked up as Mask number 0 of the Masked Swimsuit Gang when she thought Hifumi was asking them to rob a bank.\\nIt actually showed during Volume F about Ayane getting peer pressured into doing something stupid.\\nI don\\'t mind having another mentally unhinged stalker follow me around and keep me safe. And sadly I will not be chained her to Abydos as Serina, Miyako, Hina and Wakamo need me out there as well. They would not let me stay in Abydos even if I wanted to.\\nwhy we must going anywhere if we can have some *fun* with a cute ojii-san instead? \\nOh she\\'s definitely into it \\nIndeed, it is. One of my favorites.\\nKnees weak, arms are heavy\\nYou are absolutely right. If only there was a diligent student who were able to keep me safe in her room even if it\\'s \"against my will\".\\nHer palms are sweaty\\nYandere Archive is good\\nDon\\'t be crude to our old-man daughter ^(unless she\\'s into it)\\nSomething to note about Ayane is thay she\\'s very vulnerable to peer pressure if she feels there\\'s a semblance of acceptable logic.\\n\\nSo when insanity takes the rest of her friends in wanting to get SENSEI? She\\'ll follow right along with them.\\nIt\\'s Dangerous to go alone... Take this! \\n\\n*Holds out a Uweeeeee Oji-san*\\nHoshino, if you\\'re that concerned, you can just go with me.  I can\\'t complain if you\\'re the one protecting me.\\nWhy are best girls always turn into Yandere? Not that I issue but still.\\nHoshino didn\\'t want other girls near to him\\nAs it should 😑👌\\nLooks like Yandere Archive is back on the menu this new year fellas.\\nStuff like this made me realize that I wanna have toxic relationships with my Students.\\nIf you are that worried, then I will just stay by your side until you are at ease.\\nI am spiritually bound by a demonic contract to do the same 😑👌\\nI am genetically created to be attracted to yandere loli\\nThe fastest uwooghh gets the 🦀\\n\\n\\nFunniest part about how unhinged this is is I was about to say something similar but you beat me to it 😭\\nIs there even hope of getting out of this one alive\\nSolution: take hoshino\\nDamn, her mentality got so unstable it affected her ahoge.\\nAyane\\'s sanity depends in Hoshino\\'s common sense to not get dragged by Shiroko and Nonomi\\'s antics.\\n\\nAs soon as Hoshino falls and tries to keep Sensei in Abydos, Serika will follow, and Ayane will stand alone against the madness of her friends.\\nYandere Oji San only makes me more feral in wanting to absolutely violate and defile her in every possible way 😑👍\\n*Hoshino trying to restrain her yandere instinct, I see...*\\nLooks like Hoshino is getting too anxious...\\n\\nTranslation from u/theparacite\\n\\nTypeset from u/MacandCheese6\\n\\n[Source](\\n\\n[EN Twitter Ver]('"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "sub_emote_pattern = r\"!\\[img\\]\\(emote\\|t5_[a-z0-9]+\\|\\d+\\)\"\n",
    "url_pattern = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "\n",
    "patterns = [sub_emote_pattern, url_pattern]\n",
    "\n",
    "def clean(text):\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    return html.unescape(text)\n",
    "\n",
    "data_text = { i: [v.title] + v.comments for i, v in filtered_df.iterrows()}\n",
    "data_text = { i: clean('\\n'.join(post_group)) for i, post_group in data_text.items()}\n",
    "\n",
    "data_text['18vokb1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "student_list: 178\n",
      "['kayoko', 'sensei', 'master', 'arona', 'hare', 'nonomi', 'shokuhou', 'beatrice', 'iori', 'megu', 'meru', 'alice', 'kuzunoha', 'hifumi', 'yukari', 'tsurugi', 'nyanten-maru', 'phrenapates', 'saya', 'rumi', 'otogi', 'kaede', 'ibuki', 'iroha', 'eimi', 'karin', 'satsuki', 'akari', 'kai', 'sumomo', 'shiba', 'yuzu', 'makoto', 'wakamo', 'ayumu', 'haruna', 'saki', 'ruiko', 'rin', 'izumi', 'himari', 'niko', 'midori', 'ayane', 'aoi', 'gsc', 'erika', 'koharu', 'fubuki', 'kirino', 'takane', 'kisaki', 'akane', 'minori', 'hanae', 'maki', 'shinon', 'marina', 'decalcomanie', 'saten', 'reijo', 'sena', 'koyuki', 'reisa', 'renge', 'miku', 'mina', 'kaya', 'azusa', 'haruka', 'misaki', 'golconde', 'hibiki', 'mashiro', 'suzumi', 'shiroko', 'plana', 'maestro', 'kotama', 'mikoto', 'tomoe', 'sora', 'serina', 'umika', 'nagisa', 'sakurako', 'ako', 'momoka', 'niya', 'izuna', 'kurumi', 'junko', 'hiyori', 'shimiko', 'mimori', 'nodoka', 'atsuko', 'aru', 'kirara', 'hina', 'michiru', 'chihiro', 'shizuko', 'chinatsu', 'akira', 'mari', 'asuna', 'haine', 'momoi', 'kasumi', 'shigure', 'tsubaki', 'kikyou', 'nagusa', 'mai', 'yuuka', 'utaha', 'natsu', 'francis', 'airi', 'miyako', 'noa', 'chise', 'tsukuyo', 'hasumi', 'ui', 'hoshino', 'juri', 'ichika', 'cherino', 'serika', 'owner', 'moe', 'rabu', 'sumire', 'rio', 'hanako', 'kazusa', 'hinata', 'mutsuki', 'yukino', 'descartes', 'kanna', 'kokona', 'suzume', 'kotori', 'shun', 'miyu', 'saori', 'fuuka', 'seia', 'yakumo', 'pina', 'mika', 'toki', 'yoshimi', 'mine', 'misaka', 'kaho', 'neru', 'momiji', 'aris', 'aris', 'alice', 'arisu', 'fridge', 'refrigerator', 'hoshino', 'oji-san', 'ojisan', 'oji san', 'yuuka', 'yuka', 'calculator', '100kg', '100 kg', 'sensei', 'senseis']\n",
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   emoji                                                                 False      \n",
      "                                                                                     \n",
      "1   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "2   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "3   custom_boundary                                                       False      \n",
      "                                                                                     \n",
      "4   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "5   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "6   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "7   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacymoji import Emoji\n",
    "\n",
    "print(spacy.require_gpu())\n",
    "\n",
    "student_list = list()\n",
    "with open(student_path, 'r', encoding='utf-8') as f:\n",
    "    for l in f.readlines():\n",
    "        student_list.append(l.strip())\n",
    "\n",
    "# alias groups - (main name, alias-1, alias-2, alias-3, ...)\n",
    "alias_groups = [\n",
    "    ('aris', 'alice', 'arisu', 'fridge', 'refrigerator'),\n",
    "    ('hoshino', 'oji-san', 'ojisan', 'oji san'),\n",
    "    ('yuuka', 'yuka', 'calculator', '100kg', '100 kg'),\n",
    "    ('sensei', 'senseis'), # cannot be detected as plural\n",
    "]\n",
    "\n",
    "student_list.extend([j for i in alias_groups for j in i])\n",
    "\n",
    "print('student_list:', len(student_list))\n",
    "print(student_list)\n",
    "\n",
    "\n",
    "def BA_char_finder(tk):\n",
    "    if (student := tk.text.lower()) in student_list:\n",
    "        for group in alias_groups:\n",
    "            if student in group:\n",
    "                student = group[0]\n",
    "        return student\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_nbors_(tk: spacy.tokens.token.Token, step: int, N: int):\n",
    "    # step usually should be 1 or -1 - searching range step\n",
    "    # N is the Nth search result to be returned\n",
    "\n",
    "    if tk.is_sent_start and step < 0:\n",
    "        return\n",
    "    elif tk.is_sent_end and step > 0:\n",
    "        return\n",
    "    \n",
    "    n = 1\n",
    "    i = step\n",
    "    while True:\n",
    "        nbor = tk.nbor(i)\n",
    "        if nbor.is_stop or nbor._.is_emoji or not nbor.is_alpha:\n",
    "            pass\n",
    "        else:\n",
    "            if n == N:\n",
    "                return nbor\n",
    "            else:\n",
    "                n += 1\n",
    "                pass\n",
    "        \n",
    "        if nbor.is_sent_end or nbor.is_sent_start:\n",
    "            return\n",
    "\n",
    "        i += step\n",
    "\n",
    "\n",
    "def get_nbors(tk):\n",
    "    return ((\n",
    "        (\n",
    "        get_nbors_(tk, -1, 1), # l1\n",
    "        get_nbors_(tk, 1, 1), # r1\n",
    "        ),\n",
    "        (\n",
    "        get_nbors_(tk, -1, 2), # l2\n",
    "        get_nbors_(tk, 1, 2), # r2\n",
    "        ),\n",
    "        # tk.sent\n",
    "    ))\n",
    "\n",
    "# for splitting sentences\n",
    "@spacy.language.Language.component(\"custom_boundary\")\n",
    "def custom_boundary(doc):\n",
    "\n",
    "    delimiters=['...', '\\n']\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        if token.text in delimiters:\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.add_pipe('custom_boundary', before='parser')\n",
    "\n",
    "Token.set_extension(\"nbors\", getter=get_nbors, force=True)\n",
    "Token.set_extension(\"ba_characters\", getter=BA_char_finder, force=True)\n",
    "\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### planned pipeline\n",
    "\n",
    "first, use the `set_extention` to set all the student attr. might add it as a custom pipeline. since the students have alternate names, nicknames I should consider that\n",
    "\n",
    "**for getting co-occurrence matrix for students**\n",
    "1. iter through the tokens\n",
    "2. if `student` extension returns a student: start the below, else: continue\n",
    "3. get the neighboring words (maybe a range of 2 on each side, and give the closer one higher score)\n",
    "4. if the neighbor token: is a stopword(`is_stop`): pass, is an emoji(`is_emoji`): pass\n",
    "5. expand the search range until the required number of token(2 on each side) is found\n",
    "6. lowercase (and probably clean up a bit) the token and add to list with required metadata(such as score)\n",
    "7. score for each descriptive token is determined using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuffs to use\n",
    "\n",
    "**token**\n",
    "- get/set_extension\n",
    "- nbor\n",
    "- similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data_text['18vnj59'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New \tTrue / False - <0>\n",
      "Year \tFalse / False - <1>\n",
      "Mutsuki \tFalse / False - <2>\n",
      "( \tFalse / False - <3>\n",
      "by \tFalse / False - <4>\n",
      "@12beeeef \tFalse / False - <5>\n",
      ") \tFalse / False - <6>\n",
      "\n",
      " \tFalse / True - <7>\n",
      "nah \tTrue / False - <8>\n",
      ", \tFalse / False - <9>\n",
      "nai \tFalse / False - <10>\n",
      "v3 \tFalse / False - <11>\n",
      "can \tFalse / False - <12>\n",
      "already \tFalse / False - <13>\n",
      "do \tFalse / False - <14>\n",
      "halo \tFalse / False - <15>\n",
      "in \tFalse / False - <16>\n",
      "different \tFalse / False - <17>\n",
      "position \tFalse / False - <18>\n",
      "( \tFalse / False - <19>\n",
      "even \tFalse / False - <20>\n",
      "upside \tFalse / False - <21>\n",
      "- \tFalse / False - <22>\n",
      "down \tFalse / False - <23>\n",
      ") \tFalse / False - <24>\n",
      "\n",
      " \tFalse / True - <25>\n",
      "seem \tTrue / False - <26>\n",
      "like \tFalse / False - <27>\n",
      "[ \tFalse / False - <28>\n",
      "they \tFalse / False - <29>\n",
      "get \tFalse / False - <30>\n",
      "pretty \tFalse / False - <31>\n",
      "damn \tFalse / False - <32>\n",
      "close \tFalse / False - <33>\n",
      "] \tFalse / False - <34>\n",
      "( \tFalse / False - <35>\n",
      "from \tFalse / False - <36>\n",
      "what \tFalse / False - <37>\n",
      "I \tFalse / False - <38>\n",
      "just \tFalse / False - <39>\n",
      "prompt \tFalse / False - <40>\n",
      "into \tFalse / False - <41>\n",
      "NovelAI \tFalse / False - <42>\n",
      "v3 \tFalse / False - <43>\n",
      ". \tFalse / False - <44>\n",
      "\n",
      "\n",
      " \tFalse / True - <45>\n",
      "Though \tTrue / False - <46>\n",
      "Mika \tFalse / False - <47>\n",
      "'s \tFalse / False - <48>\n",
      "you \tFalse / False - <49>\n",
      "could \tFalse / False - <50>\n",
      "probably \tFalse / False - <51>\n",
      "get \tFalse / False - <52>\n",
      "away \tFalse / False - <53>\n",
      "with \tFalse / False - <54>\n",
      "since \tFalse / False - <55>\n",
      "it \tFalse / False - <56>\n",
      "probably \tFalse / False - <57>\n",
      "do \tFalse / False - <58>\n",
      "not \tFalse / False - <59>\n",
      "need \tFalse / False - <60>\n",
      "fine \tFalse / False - <61>\n",
      "angle \tFalse / False - <62>\n",
      "as \tFalse / False - <63>\n",
      "long \tFalse / False - <64>\n",
      "as \tFalse / False - <65>\n",
      "the \tFalse / False - <66>\n",
      "swirl \tFalse / False - <67>\n",
      "black \tFalse / False - <68>\n",
      "hole \tFalse / False - <69>\n",
      "be \tFalse / False - <70>\n",
      "preserve \tFalse / False - <71>\n",
      ". \tFalse / False - <72>\n",
      "\n",
      " \tFalse / True - <73>\n",
      "😭 \tTrue / False - <74>\n",
      "😭 \tFalse / False - <75>\n",
      "😭 \tFalse / False - <76>\n",
      "😭 \tFalse / False - <77>\n",
      "😭 \tFalse / False - <78>\n",
      "😭 \tFalse / False - <79>\n",
      "😭 \tFalse / False - <80>\n",
      "😭 \tFalse / False - <81>\n",
      "😭 \tFalse / False - <82>\n",
      "😭 \tFalse / False - <83>\n",
      "😭 \tFalse / False - <84>\n",
      "😭 \tFalse / False - <85>\n",
      "😭 \tFalse / False - <86>\n",
      "😭 \tFalse / False - <87>\n",
      "😭 \tFalse / False - <88>\n",
      "\n",
      " \tFalse / True - <89>\n",
      "UUUUUUOOOOOOOOOOOOOOOOOOOOH \tTrue / True - <90>\n",
      "😭 \tTrue / False - <91>\n",
      "😭 \tFalse / False - <92>\n",
      "😭 \tFalse / False - <93>\n",
      "\n",
      " \tFalse / True - <94>\n",
      "Bro \tTrue / False - <95>\n",
      "\n",
      " \tFalse / True - <96>\n",
      "probably \tTrue / False - <97>\n",
      "the \tFalse / False - <98>\n",
      "halo \tFalse / False - <99>\n",
      "be \tFalse / False - <100>\n",
      "make \tFalse / False - <101>\n",
      "by \tFalse / False - <102>\n",
      "real \tFalse / False - <103>\n",
      "human \tFalse / False - <104>\n",
      "and \tFalse / False - <105>\n",
      "the \tFalse / False - <106>\n",
      "rest \tFalse / False - <107>\n",
      "be \tFalse / False - <108>\n",
      "AI \tFalse / False - <109>\n",
      "\n",
      " \tFalse / True - <110>\n",
      "AI \tTrue / False - <111>\n",
      "have \tFalse / False - <112>\n",
      "have \tFalse / False - <113>\n",
      "hand \tFalse / False - <114>\n",
      "figure \tFalse / False - <115>\n",
      "out \tFalse / False - <116>\n",
      "for \tFalse / False - <117>\n",
      "a \tFalse / False - <118>\n",
      "while \tFalse / False - <119>\n",
      ". \tFalse / True - <120>\n",
      "it \tTrue / False - <121>\n",
      "be \tFalse / False - <122>\n",
      "only \tFalse / False - <123>\n",
      "a \tFalse / False - <124>\n",
      "tiny \tFalse / False - <125>\n",
      "bit \tFalse / False - <126>\n",
      "more \tFalse / False - <127>\n",
      "work \tFalse / False - <128>\n",
      ". \tFalse / False - <129>\n",
      "\n",
      "\n",
      " \tFalse / True - <130>\n",
      "what \tTrue / False - <131>\n",
      "surprise \tFalse / False - <132>\n",
      "I \tFalse / False - <133>\n",
      "even \tFalse / False - <134>\n",
      "more \tFalse / False - <135>\n",
      "than \tFalse / False - <136>\n",
      "the \tFalse / False - <137>\n",
      "hand \tFalse / False - <138>\n",
      "be \tFalse / False - <139>\n",
      "the \tFalse / False - <140>\n",
      "halo \tFalse / False - <141>\n",
      "on \tFalse / False - <142>\n",
      "she \tFalse / False - <143>\n",
      ". \tFalse / True - <144>\n",
      "that \tTrue / False - <145>\n",
      "be \tFalse / False - <146>\n",
      "something \tFalse / False - <147>\n",
      "a \tFalse / False - <148>\n",
      "bit \tFalse / False - <149>\n",
      "more \tFalse / False - <150>\n",
      "abstract \tFalse / False - <151>\n",
      ", \tFalse / False - <152>\n",
      "and \tFalse / False - <153>\n",
      "it \tFalse / False - <154>\n",
      "be \tFalse / False - <155>\n",
      "insane \tFalse / False - <156>\n",
      "to \tFalse / False - <157>\n",
      "I \tFalse / False - <158>\n",
      "that \tFalse / False - <159>\n",
      "the \tFalse / False - <160>\n",
      "model \tFalse / False - <161>\n",
      "have \tFalse / False - <162>\n",
      "a \tFalse / False - <163>\n",
      "solidify \tFalse / False - <164>\n",
      "concept \tFalse / False - <165>\n",
      "of \tFalse / False - <166>\n",
      "her \tFalse / False - <167>\n",
      "halo \tFalse / False - <168>\n",
      ". \tFalse / False - <169>\n",
      "\n",
      " \tFalse / True - <170>\n",
      "[ \tTrue / False - <171>\n",
      "remove \tFalse / False - <172>\n",
      "] \tFalse / False - <173>\n",
      "\n",
      " \tFalse / True - <174>\n",
      "😭 \tTrue / False - <175>\n",
      "😭 \tFalse / False - <176>\n",
      "😭 \tFalse / False - <177>\n",
      "😭 \tFalse / False - <178>\n",
      "😭 \tFalse / False - <179>\n",
      "\n",
      " \tFalse / True - <180>\n",
      "no \tTrue / False - <181>\n",
      "bra \tFalse / False - <182>\n",
      "& \tFalse / False - <183>\n",
      "pantie \tFalse / False - <184>\n",
      "on \tFalse / False - <185>\n",
      "to \tFalse / False - <186>\n",
      "greet \tFalse / False - <187>\n",
      "your \tFalse / False - <188>\n",
      "sensei \tFalse / False - <189>\n",
      "for \tFalse / False - <190>\n",
      "new \tFalse / False - <191>\n",
      "year \tFalse / False - <192>\n",
      "wish \tFalse / False - <193>\n",
      ", \tFalse / False - <194>\n",
      "ohhhh \tFalse / False - <195>\n",
      "you \tFalse / False - <196>\n",
      "smirky \tFalse / False - <197>\n",
      "sneaky \tFalse / False - <198>\n",
      "girl \tFalse / False - <199>\n",
      "\n",
      " \tFalse / True - <200>\n",
      "I \tTrue / False - <201>\n",
      "only \tFalse / False - <202>\n",
      "browse \tFalse / False - <203>\n",
      "this \tFalse / False - <204>\n",
      "sub \tFalse / False - <205>\n",
      "when \tFalse / False - <206>\n",
      "alone \tFalse / True - <207>\n",
      "lol \tTrue / False - <208>\n",
      "\n",
      " \tFalse / True - <209>\n",
      "I \tTrue / False - <210>\n",
      "can \tFalse / False - <211>\n",
      "see \tFalse / False - <212>\n",
      "it \tFalse / False - <213>\n",
      "on \tFalse / False - <214>\n",
      "her \tFalse / False - <215>\n",
      "hand \tFalse / False - <216>\n",
      ". \tFalse / True - <217>\n",
      "ai \tTrue / False - <218>\n",
      "be \tFalse / False - <219>\n",
      "reach \tFalse / False - <220>\n",
      "dangerous \tFalse / False - <221>\n",
      "level \tFalse / False - <222>\n",
      "\n",
      " \tFalse / True - <223>\n",
      "it \tTrue / False - <224>\n",
      "be \tFalse / False - <225>\n",
      "! \tFalse / True - <226>\n",
      "check \tTrue / False - <227>\n",
      "the \tFalse / False - <228>\n",
      "guy \tFalse / False - <229>\n",
      "profile \tFalse / False - <230>\n",
      ". \tFalse / True - <231>\n",
      "it \tTrue / False - <232>\n",
      "be \tFalse / False - <233>\n",
      "quite \tFalse / False - <234>\n",
      "the \tFalse / False - <235>\n",
      "goldmine \tFalse / False - <236>\n",
      ". \tFalse / False - <237>\n",
      "\n",
      " \tFalse / True - <238>\n",
      "nooooo \tTrue / False - <239>\n",
      ", \tFalse / False - <240>\n",
      "really \tFalse / False - <241>\n",
      "? \tFalse / False - <242>\n",
      "! \tFalse / False - <243>\n",
      "\n",
      " \tFalse / True - <244>\n",
      "friendly \tTrue / False - <245>\n",
      "reminder \tFalse / False - <246>\n",
      "that \tFalse / False - <247>\n",
      "this \tFalse / False - <248>\n",
      "masterpiece \tFalse / False - <249>\n",
      "be \tFalse / False - <250>\n",
      "AI \tFalse / False - <251>\n",
      "generate \tFalse / False - <252>\n",
      ", \tFalse / False - <253>\n",
      "crazy \tFalse / False - <254>\n",
      "how \tFalse / False - <255>\n",
      "much \tFalse / False - <256>\n",
      "have \tFalse / False - <257>\n",
      "improve \tFalse / False - <258>\n",
      "in \tFalse / False - <259>\n",
      "just \tFalse / False - <260>\n",
      "a \tFalse / False - <261>\n",
      "year \tFalse / False - <262>\n",
      ". \tFalse / False - <263>\n",
      "\n",
      " \tFalse / True - <264>\n",
      "😭 \tTrue / False - <265>\n",
      "Moot \tFalse / True - <266>\n",
      "😭 \tTrue / True - <267>\n",
      "moot \tTrue / True - <268>\n",
      "😭 \tTrue / False - <269>\n",
      "moot \tFalse / False - <270>\n",
      "😭 \tFalse / False - <271>\n",
      "\n",
      " \tFalse / True - <272>\n",
      "turn \tTrue / False - <273>\n",
      "she \tFalse / False - <274>\n",
      "around \tFalse / False - <275>\n",
      "for \tFalse / False - <276>\n",
      "another \tFalse / False - <277>\n",
      "page \tFalse / False - <278>\n",
      "\n",
      " \tFalse / True - <279>\n",
      "lick \tTrue / False - <280>\n",
      "the \tFalse / False - <281>\n",
      "shoulder \tFalse / False - <282>\n",
      "\n",
      " \tFalse / True - <283>\n",
      "they \tTrue / False - <284>\n",
      "will \tFalse / False - <285>\n",
      "misunderstand \tFalse / False - <286>\n",
      "and \tFalse / False - <287>\n",
      "disown \tFalse / False - <288>\n",
      "I \tFalse / False - <289>\n",
      "lol \tFalse / False - <290>\n",
      "\n",
      " \tFalse / True - <291>\n",
      "Time \tTrue / False - <292>\n",
      "to \tFalse / False - <293>\n",
      "himehajime \tFalse / False - <294>\n",
      "! \tFalse / False - <295>\n",
      "\n",
      " \tFalse / True - <296>\n",
      "make \tTrue / False - <297>\n",
      "your \tFalse / False - <298>\n",
      "family \tFalse / False - <299>\n",
      "the \tFalse / False - <300>\n",
      "Senseis \tFalse / False - <301>\n",
      "of \tFalse / False - <302>\n",
      "tomorrow \tFalse / False - <303>\n",
      "\n",
      " \tFalse / True - <304>\n",
      "turn \tTrue / False - <305>\n",
      "around \tFalse / False - <306>\n",
      ", \tFalse / False - <307>\n",
      "brat \tFalse / False - <308>\n",
      ". \tFalse / True - <309>\n",
      "I \tTrue / False - <310>\n",
      "have \tFalse / False - <311>\n",
      "to \tFalse / False - <312>\n",
      "finish \tFalse / False - <313>\n",
      "quick \tFalse / False - <314>\n",
      ". \tFalse / True - <315>\n",
      "Rin \tTrue / False - <316>\n",
      "will \tFalse / False - <317>\n",
      "kill \tFalse / False - <318>\n",
      "I \tFalse / False - <319>\n",
      "if \tFalse / False - <320>\n",
      "she \tFalse / False - <321>\n",
      "see \tFalse / False - <322>\n",
      "I \tFalse / False - <323>\n",
      "nut \tFalse / False - <324>\n",
      "on \tFalse / False - <325>\n",
      "the \tFalse / False - <326>\n",
      "job \tFalse / False - <327>\n",
      ". \tFalse / False - <328>\n",
      "\n",
      " \tFalse / True - <329>\n",
      "* \tTrue / False - <330>\n",
      "you \tFalse / False - <331>\n",
      "get \tFalse / False - <332>\n",
      "what \tFalse / False - <333>\n",
      "I \tFalse / False - <334>\n",
      "have \tFalse / False - <335>\n",
      "say \tFalse / False - <336>\n",
      ". \tFalse / True - <337>\n",
      "* \tTrue / False - <338>\n",
      "\n",
      " \tFalse / True - <339>\n",
      "this \tTrue / False - <340>\n",
      "brat \tFalse / False - <341>\n",
      "’ \tFalse / False - <342>\n",
      "back \tFalse / False - <343>\n",
      "be \tFalse / False - <344>\n",
      "too \tFalse / False - <345>\n",
      "tempe \tFalse / False - <346>\n",
      "to \tFalse / False - <347>\n",
      "sensei \tFalse / False - <348>\n",
      "😭 \tFalse / False - <349>\n",
      "😭 \tFalse / False - <350>\n",
      "😭 \tFalse / False - <351>\n",
      ". \tFalse / True - <352>\n",
      "she \tTrue / False - <353>\n",
      "can \tFalse / False - <354>\n",
      "not \tFalse / False - <355>\n",
      "keep \tFalse / False - <356>\n",
      "tease \tFalse / False - <357>\n",
      "sensei \tFalse / False - <358>\n",
      "like \tFalse / False - <359>\n",
      "this \tFalse / False - <360>\n",
      "otherwise \tFalse / False - <361>\n",
      "they \tFalse / False - <362>\n",
      "’ll \tFalse / False - <363>\n",
      "act \tFalse / False - <364>\n",
      "unwise \tFalse / False - <365>\n",
      "\n",
      " \tFalse / True - <366>\n",
      "Ayo \tTrue / False - <367>\n",
      "? \tFalse / False - <368>\n",
      "\n",
      " \tFalse / True - <369>\n",
      "I \tTrue / False - <370>\n",
      "should \tFalse / False - <371>\n",
      "not \tFalse / False - <372>\n",
      "be \tFalse / False - <373>\n",
      "on \tFalse / False - <374>\n",
      "this \tFalse / False - <375>\n",
      "subreddit \tFalse / False - <376>\n",
      "when \tFalse / False - <377>\n",
      "around \tFalse / False - <378>\n",
      "family \tFalse / False - <379>\n",
      "lol \tFalse / False - <380>\n",
      "\n",
      "\n",
      " \tFalse / False - <381>\n",
      "for \tFalse / False - <382>\n",
      "easy \tFalse / False - <383>\n",
      "access \tFalse / False - <384>\n",
      ". \tFalse / False - <385>\n",
      "\n",
      " \tFalse / True - <386>\n",
      "# \tTrue / False - <387>\n",
      "WA \tFalse / False - <388>\n",
      "- \tFalse / False - <389>\n",
      "WA \tFalse / False - <390>\n",
      "- \tFalse / False - <391>\n",
      "WAH \tFalse / False - <392>\n",
      "! \tFalse / False - <393>\n",
      "MUTSUKI \tFalse / False - <394>\n",
      "'S \tFalse / False - <395>\n",
      "back \tFalse / False - <396>\n",
      "! \tFalse / False - <397>\n",
      "\n",
      " \tFalse / True - <398>\n",
      "* \tTrue / False - <399>\n",
      "* \tFalse / False - <400>\n",
      "confession \tFalse / False - <401>\n",
      "towards \tFalse / False - <402>\n",
      "Mutsuki \tFalse / False - <403>\n",
      "- \tFalse / False - <404>\n",
      "chan \tFalse / False - <405>\n",
      "* \tFalse / False - <406>\n",
      "* \tFalse / False - <407>\n",
      "\n",
      "\n",
      " \tFalse / False - <408>\n",
      "oh \tFalse / False - <409>\n",
      ", \tFalse / False - <410>\n",
      "Mutsuki \tFalse / False - <411>\n",
      "! \tFalse / True - <412>\n",
      "I \tTrue / False - <413>\n",
      "can \tFalse / False - <414>\n",
      "not \tFalse / False - <415>\n",
      "take \tFalse / False - <416>\n",
      "my \tFalse / False - <417>\n",
      "eye \tFalse / False - <418>\n",
      "off \tFalse / False - <419>\n",
      "you \tFalse / False - <420>\n",
      "😳 \tFalse / True - <421>\n",
      "every \tTrue / False - <422>\n",
      "time \tFalse / False - <423>\n",
      "I \tFalse / False - <424>\n",
      "see \tFalse / False - <425>\n",
      "you \tFalse / False - <426>\n",
      ", \tFalse / False - <427>\n",
      "the \tFalse / False - <428>\n",
      "first \tFalse / False - <429>\n",
      "thing \tFalse / False - <430>\n",
      "I \tFalse / False - <431>\n",
      "look \tFalse / False - <432>\n",
      "at \tFalse / False - <433>\n",
      "your \tFalse / False - <434>\n",
      "beautiful \tFalse / False - <435>\n",
      "face \tFalse / False - <436>\n",
      ", \tFalse / False - <437>\n",
      "then \tFalse / False - <438>\n",
      "your \tFalse / False - <439>\n",
      "gorgeous \tFalse / False - <440>\n",
      "flat \tFalse / False - <441>\n",
      "body \tFalse / False - <442>\n",
      "😭 \tFalse / False - <443>\n",
      "I \tFalse / False - <444>\n",
      "wanna \tFalse / False - <445>\n",
      "cuddle \tFalse / False - <446>\n",
      "that \tFalse / False - <447>\n",
      "flat \tFalse / False - <448>\n",
      "body \tFalse / False - <449>\n",
      "of \tFalse / False - <450>\n",
      "yours \tFalse / False - <451>\n",
      "as \tFalse / False - <452>\n",
      "we \tFalse / False - <453>\n",
      "lie \tFalse / False - <454>\n",
      "down \tFalse / False - <455>\n",
      "on \tFalse / False - <456>\n",
      "the \tFalse / False - <457>\n",
      "open \tFalse / False - <458>\n",
      "field \tFalse / False - <459>\n",
      ", \tFalse / False - <460>\n",
      "look \tFalse / False - <461>\n",
      "at \tFalse / False - <462>\n",
      "the \tFalse / False - <463>\n",
      "beautiful \tFalse / False - <464>\n",
      "sky \tFalse / False - <465>\n",
      "night \tFalse / False - <466>\n",
      "🌝 \tFalse / False - <467>\n",
      "I \tFalse / False - <468>\n",
      "want \tFalse / False - <469>\n",
      "you \tFalse / False - <470>\n",
      "to \tFalse / False - <471>\n",
      "caress \tFalse / False - <472>\n",
      "my \tFalse / False - <473>\n",
      "body \tFalse / False - <474>\n",
      ", \tFalse / False - <475>\n",
      "touch \tFalse / False - <476>\n",
      "every \tFalse / False - <477>\n",
      "part \tFalse / False - <478>\n",
      "of \tFalse / False - <479>\n",
      "I \tFalse / False - <480>\n",
      "and \tFalse / False - <481>\n",
      "as \tFalse / False - <482>\n",
      "well \tFalse / False - <483>\n",
      "as \tFalse / False - <484>\n",
      "I \tFalse / False - <485>\n",
      "do \tFalse / False - <486>\n",
      "the \tFalse / False - <487>\n",
      "same \tFalse / False - <488>\n",
      "to \tFalse / False - <489>\n",
      "you \tFalse / False - <490>\n",
      "☺️ \tFalse / False - <491>\n",
      "I \tFalse / False - <492>\n",
      "want \tFalse / False - <493>\n",
      "to \tFalse / False - <494>\n",
      "hear \tFalse / False - <495>\n",
      "your \tFalse / False - <496>\n",
      "laughter \tFalse / False - <497>\n",
      "every \tFalse / False - <498>\n",
      "time \tFalse / False - <499>\n",
      "you \tFalse / False - <500>\n",
      "prank \tFalse / False - <501>\n",
      "I \tFalse / False - <502>\n",
      ", \tFalse / False - <503>\n",
      "treat \tFalse / False - <504>\n",
      "I \tFalse / False - <505>\n",
      "nothing \tFalse / False - <506>\n",
      "but \tFalse / False - <507>\n",
      "a \tFalse / False - <508>\n",
      "toy \tFalse / False - <509>\n",
      "🥹 \tFalse / False - <510>\n",
      "I \tFalse / False - <511>\n",
      "can \tFalse / False - <512>\n",
      "be \tFalse / False - <513>\n",
      "your \tFalse / False - <514>\n",
      "personal \tFalse / False - <515>\n",
      "toy \tFalse / False - <516>\n",
      ", \tFalse / False - <517>\n",
      "Mutsuki \tFalse / False - <518>\n",
      "! \tFalse / True - <519>\n",
      "I \tTrue / False - <520>\n",
      "will \tFalse / False - <521>\n",
      "never \tFalse / False - <522>\n",
      "hate \tFalse / False - <523>\n",
      "you \tFalse / False - <524>\n",
      "for \tFalse / False - <525>\n",
      "treat \tFalse / False - <526>\n",
      "I \tFalse / False - <527>\n",
      "like \tFalse / False - <528>\n",
      "one \tFalse / False - <529>\n",
      "! \tFalse / True - <530>\n",
      "I \tTrue / False - <531>\n",
      "be \tFalse / False - <532>\n",
      "ready \tFalse / False - <533>\n",
      "to \tFalse / False - <534>\n",
      "fulfil \tFalse / False - <535>\n",
      "my \tFalse / False - <536>\n",
      "duty \tFalse / False - <537>\n",
      "as \tFalse / False - <538>\n",
      "your \tFalse / False - <539>\n",
      "future \tFalse / False - <540>\n",
      "husband \tFalse / False - <541>\n",
      "✨ \tFalse / False - <542>\n",
      "I \tFalse / False - <543>\n",
      "will \tFalse / False - <544>\n",
      "share \tFalse / False - <545>\n",
      "everything \tFalse / False - <546>\n",
      "I \tFalse / False - <547>\n",
      "have \tFalse / False - <548>\n",
      "with \tFalse / False - <549>\n",
      "you \tFalse / False - <550>\n",
      ", \tFalse / False - <551>\n",
      "Mutsuki \tFalse / False - <552>\n",
      "! \tFalse / True - <553>\n",
      "I \tTrue / False - <554>\n",
      "love \tFalse / False - <555>\n",
      "you \tFalse / False - <556>\n",
      ", \tFalse / False - <557>\n",
      "Mutsuki \tFalse / False - <558>\n",
      "! \tFalse / False - <559>\n",
      "💕 \tFalse / False - <560>\n",
      "\n",
      "\n",
      " \tFalse / False - <561>\n",
      "Mutsuki \tFalse / False - <562>\n",
      "'s \tFalse / False - <563>\n",
      "back \tFalse / False - <564>\n",
      "be \tFalse / False - <565>\n",
      "so \tFalse / True - <566>\n",
      "er0tic \tTrue / False - <567>\n",
      "😭 \tFalse / False - <568>\n",
      "😭 \tFalse / False - <569>\n",
      "😭 \tFalse / False - <570>\n",
      "I \tFalse / False - <571>\n",
      "wanna \tFalse / False - <572>\n",
      "lick \tFalse / False - <573>\n",
      "it~ \tFalse / False - <574>\n",
      "💦 \tFalse / False - <575>\n",
      "\n",
      " \tFalse / True - <576>\n",
      "those \tTrue / False - <577>\n",
      "SMOOTH \tFalse / False - <578>\n",
      "SOFT \tFalse / False - <579>\n",
      "SHOULDER \tFalse / False - <580>\n",
      "BLADES \tFalse / True - <581>\n",
      "😭 \tTrue / False - <582>\n",
      "😭 \tFalse / False - <583>\n",
      "😭 \tFalse / False - <584>\n",
      "😭 \tFalse / False - <585>\n",
      "😭 \tFalse / False - <586>\n",
      "\n",
      " \tFalse / True - <587>\n",
      "you \tTrue / False - <588>\n",
      "be \tFalse / False - <589>\n",
      "the \tFalse / False - <590>\n",
      "one \tFalse / False - <591>\n",
      "that \tFalse / False - <592>\n",
      "GOT \tFalse / False - <593>\n",
      "ME \tFalse / False - <594>\n",
      "BANNED \tFalse / False - <595>\n",
      "🚫 \tFalse / False - <596>\n",
      "for \tFalse / False - <597>\n",
      "a \tFalse / False - <598>\n",
      "week \tFalse / False - <599>\n",
      ", \tFalse / False - <600>\n",
      "MUTSUKI \tFalse / False - <601>\n",
      "- \tFalse / False - <602>\n",
      "chan \tFalse / False - <603>\n",
      "😭 \tFalse / False - <604>\n",
      "😭 \tFalse / False - <605>\n",
      "😭 \tFalse / False - <606>\n",
      "💢 \tFalse / False - <607>\n",
      "💢 \tFalse / False - <608>\n",
      "💢 \tFalse / False - <609>\n",
      "💢 \tFalse / False - <610>\n",
      "that \tFalse / False - <611>\n",
      "CURRENTLY \tFalse / False - <612>\n",
      "loose \tFalse / False - <613>\n",
      "outfit \tFalse / False - <614>\n",
      "...... \tFalse / False - <615>\n",
      "😠 \tFalse / False - <616>\n",
      "😠 \tFalse / False - <617>\n",
      "😠 \tFalse / False - <618>\n",
      "😠 \tFalse / False - <619>\n",
      "💢 \tFalse / False - <620>\n",
      "💢 \tFalse / False - <621>\n",
      "💢 \tFalse / False - <622>\n",
      "💢 \tFalse / False - <623>\n",
      "💢 \tFalse / False - <624>\n",
      "that \tFalse / False - <625>\n",
      "BARE \tFalse / False - <626>\n",
      "back \tFalse / False - <627>\n",
      "😡 \tFalse / False - <628>\n",
      "😡 \tFalse / False - <629>\n",
      "😡 \tFalse / False - <630>\n",
      "😡 \tFalse / False - <631>\n",
      "😡 \tFalse / False - <632>\n",
      "😡 \tFalse / False - <633>\n",
      "💢 \tFalse / False - <634>\n",
      "💢 \tFalse / False - <635>\n",
      "💢 \tFalse / False - <636>\n",
      "💢 \tFalse / False - <637>\n",
      "💢 \tFalse / False - <638>\n",
      "plus \tFalse / False - <639>\n",
      "that \tFalse / False - <640>\n",
      "SMUG \tFalse / False - <641>\n",
      ", \tFalse / False - <642>\n",
      "CHEEKY \tFalse / False - <643>\n",
      "FACE \tFalse / False - <644>\n",
      "👿 \tFalse / True - <645>\n",
      "👿 \tTrue / True - <646>\n",
      "👿 \tTrue / True - <647>\n",
      "👿 \tTrue / True - <648>\n",
      "👿 \tTrue / False - <649>\n",
      "👿 \tFalse / False - <650>\n",
      "👿 \tFalse / False - <651>\n",
      "💢 \tFalse / False - <652>\n",
      "💢 \tFalse / False - <653>\n",
      "💢 \tFalse / False - <654>\n",
      "💢 \tFalse / False - <655>\n",
      "💢 \tFalse / False - <656>\n",
      "💢 \tFalse / False - <657>\n",
      "💢 \tFalse / False - <658>\n",
      "I'LL \tFalse / True - <659>\n",
      "never \tTrue / False - <660>\n",
      "forgive \tFalse / False - <661>\n",
      "you \tFalse / False - <662>\n",
      "for \tFalse / False - <663>\n",
      "derail \tFalse / False - <664>\n",
      "any \tFalse / False - <665>\n",
      "and \tFalse / False - <666>\n",
      "all \tFalse / False - <667>\n",
      "plan \tFalse / False - <668>\n",
      "📍 \tFalse / False - <669>\n",
      "for \tFalse / False - <670>\n",
      "SENSEI \tFalse / False - <671>\n",
      "🧑‍🏫 \tFalse / False - <672>\n",
      "to \tFalse / False - <673>\n",
      "help \tFalse / False - <674>\n",
      "🆘 \tFalse / True - <675>\n",
      "his \tTrue / False - <676>\n",
      "student \tFalse / False - <677>\n",
      "with \tFalse / False - <678>\n",
      "their \tFalse / False - <679>\n",
      "INDIVIDUAL \tFalse / False - <680>\n",
      "needs \tFalse / False - <681>\n",
      "and \tFalse / False - <682>\n",
      "want \tFalse / False - <683>\n",
      "😤 \tFalse / False - <684>\n",
      "😤 \tFalse / False - <685>\n",
      "😤 \tFalse / False - <686>\n",
      "😤 \tFalse / False - <687>\n",
      "😤 \tFalse / False - <688>\n",
      "😤 \tFalse / False - <689>\n",
      "😤 \tFalse / False - <690>\n",
      "😤 \tFalse / False - <691>\n",
      "😤 \tFalse / False - <692>\n",
      "😤 \tFalse / False - <693>\n",
      "👺 \tFalse / False - <694>\n",
      "👺 \tFalse / False - <695>\n",
      "👺 \tFalse / False - <696>\n",
      "👺 \tFalse / False - <697>\n",
      "👺 \tFalse / False - <698>\n",
      "👺 \tFalse / False - <699>\n",
      "👺 \tFalse / False - <700>\n",
      "👺 \tFalse / False - <701>\n",
      "👺 \tFalse / False - <702>\n",
      "👹 \tFalse / False - <703>\n",
      "👹 \tFalse / False - <704>\n",
      "👹 \tFalse / False - <705>\n",
      "👹 \tFalse / False - <706>\n",
      "👹 \tFalse / False - <707>\n",
      "👹 \tFalse / False - <708>\n",
      "👹 \tFalse / False - <709>\n",
      "👹 \tFalse / False - <710>\n",
      "💢 \tFalse / False - <711>\n",
      "💢 \tFalse / False - <712>\n",
      "💢 \tFalse / False - <713>\n",
      "💢 \tFalse / False - <714>\n",
      "💢 \tFalse / False - <715>\n",
      "💢 \tFalse / False - <716>\n",
      "💢 \tFalse / False - <717>\n",
      "💢 \tFalse / False - <718>\n",
      "💢 \tFalse / False - <719>\n",
      "💢 \tFalse / False - <720>\n",
      "\n",
      " \tFalse / True - <721>\n",
      "New \tTrue / False - <722>\n",
      "year \tFalse / False - <723>\n",
      "coitus \tFalse / False - <724>\n",
      "😭 \tFalse / False - <725>\n",
      "💢 \tFalse / False - <726>\n",
      "😭 \tFalse / False - <727>\n",
      "💢 \tFalse / False - <728>\n",
      "😭 \tFalse / False - <729>\n",
      "💢 \tFalse / False - <730>\n",
      "\n",
      " \tFalse / True - <731>\n",
      "WOOOOHWAHWAHWAAH \tTrue / False - <732>\n",
      "MUTSUKI \tFalse / False - <733>\n",
      "SPINE \tFalse / False - <734>\n",
      "💢 \tFalse / False - <735>\n",
      "💢 \tFalse / False - <736>\n",
      "\n",
      " \tFalse / True - <737>\n",
      "NEURON \tTrue / False - <738>\n",
      "ACTIVATION \tFalse / False - <739>\n",
      "... \tFalse / True - <740>\n",
      "👺 \tTrue / True - <741>\n",
      "👺 \tTrue / False - <742>\n",
      "👺 \tFalse / False - <743>\n",
      "💢 \tFalse / False - <744>\n",
      "\n",
      "\n",
      " \tFalse / False - <745>\n",
      "🚨 \tFalse / False - <746>\n",
      "BRAT \tFalse / False - <747>\n",
      "correction \tFalse / False - <748>\n",
      "PROTOCOL \tFalse / False - <749>\n",
      "engage \tFalse / False - <750>\n",
      "🚨 \tFalse / False - <751>\n",
      "\n",
      "\n",
      " \tFalse / False - <752>\n",
      "I \tFalse / False - <753>\n",
      "will \tFalse / False - <754>\n",
      "LICK \tFalse / False - <755>\n",
      "every \tFalse / False - <756>\n",
      "INCH \tFalse / False - <757>\n",
      "of \tFalse / False - <758>\n",
      "skin \tFalse / False - <759>\n",
      "in \tFalse / False - <760>\n",
      "her \tFalse / False - <761>\n",
      "spine \tFalse / False - <762>\n",
      "and \tFalse / False - <763>\n",
      "then \tFalse / False - <764>\n",
      "INTRODUCE \tFalse / False - <765>\n",
      "mutsuki \tFalse / False - <766>\n",
      "to \tFalse / False - <767>\n",
      "PLEASURE \tFalse / False - <768>\n",
      "unknown \tFalse / False - <769>\n",
      "to \tFalse / False - <770>\n",
      "she \tFalse / False - <771>\n",
      "to \tFalse / False - <772>\n",
      "make \tFalse / False - <773>\n",
      "she \tFalse / False - <774>\n",
      "mind \tFalse / False - <775>\n",
      "MORE \tFalse / False - <776>\n",
      "malleable \tFalse / False - <777>\n",
      "and \tFalse / False - <778>\n",
      "amenable \tFalse / False - <779>\n",
      "to \tFalse / False - <780>\n",
      "the \tFalse / False - <781>\n",
      "idea \tFalse / False - <782>\n",
      "of \tFalse / False - <783>\n",
      "be \tFalse / False - <784>\n",
      "a \tFalse / False - <785>\n",
      "good \tFalse / False - <786>\n",
      "girl \tFalse / False - <787>\n",
      "💢 \tFalse / False - <788>\n",
      "💢 \tFalse / False - <789>\n",
      "💢 \tFalse / False - <790>\n",
      "💢 \tFalse / False - <791>\n",
      "💢 \tFalse / False - <792>\n",
      "😭 \tFalse / False - <793>\n",
      "😭 \tFalse / False - <794>\n",
      "😭 \tFalse / False - <795>\n",
      "👺 \tFalse / False - <796>\n",
      "👺 \tFalse / False - <797>\n",
      "👺 \tFalse / False - <798>\n",
      "👺 \tFalse / False - <799>\n",
      "👺 \tFalse / False - <800>\n",
      "👺 \tFalse / False - <801>\n",
      "the \tFalse / False - <802>\n",
      "YUKATA \tFalse / False - <803>\n",
      "stay \tFalse / False - <804>\n",
      "* \tFalse / False - <805>\n",
      "* \tFalse / False - <806>\n",
      "ON \tFalse / False - <807>\n",
      "* \tFalse / False - <808>\n",
      "* \tFalse / False - <809>\n",
      "during \tFalse / False - <810>\n",
      "correction \tFalse / False - <811>\n",
      "😭 \tFalse / False - <812>\n",
      "😭 \tFalse / False - <813>\n",
      "😭 \tFalse / False - <814>\n",
      "😭 \tFalse / False - <815>\n",
      "💢 \tFalse / False - <816>\n",
      "\n",
      " \tFalse / True - <817>\n",
      "Peak \tTrue / False - <818>\n",
      ". \tFalse / False - <819>\n",
      "\n",
      " \tFalse / True - <820>\n",
      "* \tTrue / False - <821>\n",
      "pull \tFalse / False - <822>\n",
      "out \tFalse / False - <823>\n",
      "Ink \tFalse / False - <824>\n",
      "Brush \tFalse / False - <825>\n",
      "* \tFalse / False - <826>\n",
      "\n",
      "\n",
      " \tFalse / False - <827>\n",
      "Hehhehe \tFalse / False - <828>\n",
      "flat \tFalse / False - <829>\n",
      "canvas \tFalse / False - <830>\n",
      "\n",
      " \tFalse / True - <831>\n",
      "* \tTrue / True - <832>\n",
      "I \tTrue / False - <833>\n",
      "wonder \tFalse / False - <834>\n",
      "if \tFalse / False - <835>\n",
      "have \tFalse / False - <836>\n",
      "a \tFalse / False - <837>\n",
      "little \tFalse / False - <838>\n",
      "one \tFalse / False - <839>\n",
      "of \tFalse / False - <840>\n",
      "her \tFalse / False - <841>\n",
      "own \tFalse / False - <842>\n",
      "would \tFalse / False - <843>\n",
      "finally \tFalse / False - <844>\n",
      "get \tFalse / False - <845>\n",
      "she \tFalse / False - <846>\n",
      "correct \tFalse / False - <847>\n",
      "... \tFalse / True - <848>\n",
      "* \tTrue / False - <849>\n",
      "\n",
      " \tFalse / True - <850>\n",
      "* \tTrue / True - <851>\n",
      "so \tTrue / False - <852>\n",
      "flat \tFalse / False - <853>\n",
      ", \tFalse / False - <854>\n",
      "they \tFalse / False - <855>\n",
      "do \tFalse / False - <856>\n",
      "not \tFalse / False - <857>\n",
      "even \tFalse / False - <858>\n",
      "need \tFalse / False - <859>\n",
      "support \tFalse / False - <860>\n",
      "* \tFalse / False - <861>\n",
      "\n",
      " \tFalse / True - <862>\n",
      "[ \tTrue / False - <863>\n",
      "source \tFalse / False - <864>\n",
      "] \tFalse / False - <865>\n",
      "( \tFalse / True - <866>\n"
     ]
    }
   ],
   "source": [
    "for i, tk in enumerate(doc):\n",
    "    print(f'{tk.lemma_} \\t{tk.is_sent_start} / {tk.is_sent_end} - <{i}>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('😭', 'loudly crying face'), 52),\n",
       " (('💢', 'anger symbol'), 43),\n",
       " (('👺', 'goblin'), 18),\n",
       " (('😤', 'face with steam from nose'), 10),\n",
       " (('👹', 'ogre'), 8),\n",
       " (('👿', 'angry face with horns'), 7),\n",
       " (('😡', 'enraged face'), 6),\n",
       " (('😠', 'angry face'), 4),\n",
       " (('🚨', 'police car light'), 2),\n",
       " (('😳', 'flushed face'), 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([(emote[0], emote[2]) for emote in doc._.emoji]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'mutsuki': 11, 'sensei': 4, 'mika': 1, 'rin': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([tk._.ba_characters for tk in doc if tk._.ba_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((greet, new), (panties, year)),\n",
       " ((temping, None), (brat, None)),\n",
       " ((teasing, like), (None, act)),\n",
       " ((PLANS, HELP), (DERAILING, None))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tk._.nbors for tk in doc if tk._.ba_characters == 'sensei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scoring (default embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce11d0d0e54604a9334749ada69473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try: del doc\n",
    "except: pass\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "docs = list()\n",
    "for t in tqdm(data_text.values()):\n",
    "    docs.append(nlp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sensei', 91),\n",
       " ('arona', 31),\n",
       " ('yuuka', 27),\n",
       " ('himari', 24),\n",
       " ('hoshino', 23),\n",
       " ('hina', 22),\n",
       " ('plana', 22),\n",
       " ('rio', 17),\n",
       " ('mutsuki', 16),\n",
       " ('aru', 13)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([tk._.ba_characters for doc in docs for tk in doc if tk._.ba_characters]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pulls', 2), ('year', 2), ('chan', 2), ('mutsuki', 2), ('ny', 1), ('previous', 1), ('haruka', 1), ('wah', 1), ('confession', 1), ('oh', 1)]\n",
      "[('new', 2), ('aru', 1), ('ny', 1), ('kayoko', 1), ('help', 1), ('wa', 1), ('oh', 1), ('chan', 1), ('personal', 1), ('husband', 1)]\n"
     ]
    }
   ],
   "source": [
    "target_character = 'mutsuki'\n",
    "\n",
    "# `cm` as in correlation matrix (not exactly a matrix but...)\n",
    "\n",
    "cm_original = [tk._.nbors for doc in docs for tk in doc if tk._.ba_characters == target_character]\n",
    "cm_original_1 = [j for i in cm_original for j in i[0] if j] # bigram range\n",
    "cm_original_2 = [j for i in cm_original for j in i[1] if j] # trigram range\n",
    "\n",
    "print(Counter([i.text.lower() for i in cm_original_1]).most_common(10))\n",
    "print(Counter([i.text.lower() for i in cm_original_2]).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hard to do vector similarity because `en_core_web_lg` doesn't have vector values for some of the BA_characters**\n",
    "\n",
    "also, not sure if the vector values will actually represent the character\n",
    "\n",
    "to note:\n",
    ">SpaCy's native models, such as en_core_web_lg, the position of a word in a sentence does not affect its vector representation. These models use static word embeddings, where each word is assigned a fixed vector based on the word itself, regardless of its position or the context in which it appears.\n",
    "\n",
    "so it's okay to de-dupluicate all the words with the same `Token.text` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Mutsuki': 12, 'MUTSUKI': 4})\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m targets \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m dedup\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_vector\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`cm_original` no vector:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m [\u001b[38;5;28mprint\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m cm_original_1 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_vector]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "targets = [tk for doc in docs for tk in doc if tk._.ba_characters == target_character]\n",
    "print(Counter([t.text for t in targets]))\n",
    "\n",
    "# remove duplicate tokens for targets\n",
    "dedup = {\n",
    "    target.text: target for target in targets \n",
    "}\n",
    "targets = [t for t in dedup.values()]\n",
    "\n",
    "for t in targets:\n",
    "    assert t.has_vector, f\"{t.text} doesn't have a vector value\"\n",
    "\n",
    "print('`cm_original` no vector:')\n",
    "[print(t) for t in cm_original_1 if not t.has_vector]\n",
    "\n",
    "vec_sim = {target.text: {tk.text: tk.similarity(target) for tk in cm_original_1 if tk.has_vector} for target in tqdm(targets) if target.has_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sensei', 'sensei', 'SENSEI']\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'happiness': 0.40256187319755554,\n",
       " 'feel': 0.3986526429653168,\n",
       " 'body': 0.3569816052913666,\n",
       " 'harmony': 0.3567489683628082,\n",
       " 'respect': 0.35223153233528137,\n",
       " 'life': 0.3454945385456085,\n",
       " 'Eimi': 0.3207196295261383,\n",
       " 'interest': 0.30065616965293884,\n",
       " 'little': 0.30037540197372437,\n",
       " 'meant': 0.3002455532550812,\n",
       " 'like': 0.30020174384117126,\n",
       " 'logical': 0.2966761589050293,\n",
       " 'wanting': 0.28550535440444946,\n",
       " 'Seeing': 0.27988341450691223,\n",
       " 'loving': 0.2793347239494324,\n",
       " 'strong': 0.2783357501029968,\n",
       " 'chance': 0.2764001488685608,\n",
       " 'teasing': 0.27630293369293213,\n",
       " 'tries': 0.2691846787929535,\n",
       " 'maybe': 0.2691352069377899,\n",
       " 'work': 0.26856309175491333,\n",
       " 'moves': 0.26222139596939087,\n",
       " 'reveal': 0.254867821931839,\n",
       " 'look': 0.2531861364841461,\n",
       " 'want': 0.25263577699661255,\n",
       " 'needs': 0.24460053443908691,\n",
       " 'erotic': 0.2419493943452835,\n",
       " 'student': 0.24087487161159515,\n",
       " 'corrects': 0.23649294674396515,\n",
       " 'try': 0.22683261334896088,\n",
       " 'time': 0.22679170966148376,\n",
       " 'students': 0.22602684795856476,\n",
       " 'suffer': 0.2238903045654297,\n",
       " 'Valkyrie': 0.2230508029460907,\n",
       " 'answers': 0.2214997261762619,\n",
       " 'temping': 0.2144758701324463,\n",
       " 'comes': 0.21181312203407288,\n",
       " 'times': 0.20023560523986816,\n",
       " 'peace': 0.19641275703907013,\n",
       " 'help': 0.1954721361398697,\n",
       " 'having': 0.19411315023899078,\n",
       " 'eating': 0.1931891292333603,\n",
       " 'Impression': 0.19288134574890137,\n",
       " 'usual': 0.1922544240951538,\n",
       " 'bright': 0.19089259207248688,\n",
       " 'gets': 0.18788261711597443,\n",
       " 'comments': 0.1805015355348587,\n",
       " 'responsible': 0.18008336424827576,\n",
       " 'let': 0.17129693925380707,\n",
       " 'partake': 0.16803283989429474,\n",
       " 'handling': 0.1658923476934433,\n",
       " 'bringing': 0.16210338473320007,\n",
       " 'nt': 0.15835416316986084,\n",
       " 'coming': 0.1477719247341156,\n",
       " 'gift': 0.14416180551052094,\n",
       " 'nice': 0.1437654197216034,\n",
       " 'Himari': 0.14144675433635712,\n",
       " 'greet': 0.13294416666030884,\n",
       " 'thank': 0.13264510035514832,\n",
       " 'brought': 0.11991172283887863,\n",
       " 'PLANS': 0.11944064497947693,\n",
       " 'fishing': 0.11886486411094666,\n",
       " 'warm': 0.11885412782430649,\n",
       " 'BS': 0.11847449094057083,\n",
       " 'allows': 0.1175585612654686,\n",
       " 'best': 0.11619231849908829,\n",
       " 'dummy': 0.11054433137178421,\n",
       " 'fellow': 0.10608048737049103,\n",
       " 'dating': 0.1011698991060257,\n",
       " 'gone': 0.09854268282651901,\n",
       " 'nursed': 0.09775660187005997,\n",
       " 'new': 0.09018442779779434,\n",
       " 'stay': 0.0884479507803917,\n",
       " 'fall': 0.08589750528335571,\n",
       " 'welcome': 0.08203136920928955,\n",
       " 'lets': 0.07811473309993744,\n",
       " 'meal': 0.05315351113677025,\n",
       " 'saw': 0.04371693730354309,\n",
       " 'second': 0.04108346998691559,\n",
       " 'Gambling': 0.0405343659222126,\n",
       " 'cuddle': 0.03921762853860855,\n",
       " 'big': 0.03584767132997513,\n",
       " 'Abydos': 0.023176804184913635,\n",
       " 'Correction': 0.015568324364721775,\n",
       " 'HELP': 0.01341058500111103,\n",
       " 'date': 0.007772755343466997,\n",
       " 'LOVE': 0.007370288483798504,\n",
       " 'HAPPENED': 0.006617951672524214,\n",
       " 'FILL': 3.0591414542868733e-05,\n",
       " 'HOPES': -0.0017083421116694808,\n",
       " 'SENSE': -0.005554318893700838,\n",
       " 'FIELD': -0.009475694969296455,\n",
       " 'Gang': -0.009790276177227497,\n",
       " 'FAR': -0.013002526946365833,\n",
       " 'lays': -0.014198178425431252,\n",
       " 'busy': -0.016128089278936386,\n",
       " 'NEEDING': -0.016884686425328255,\n",
       " 'GRATEFUL': -0.01963723637163639,\n",
       " 'shortly': -0.020884431898593903,\n",
       " 'year': -0.02256271056830883,\n",
       " 'MARKETING': -0.04375869780778885,\n",
       " 'STALL': -0.04815118759870529,\n",
       " 'FOX': -0.05913008376955986,\n",
       " 'Year': -0.13886074721813202}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([k for k in vec_sim])\n",
    "print(len(vec_sim))\n",
    "{k: v for k, v in sorted(vec_sim['sensei'].items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FastText to deal with OOV tokens\n",
    "above scoring and text2vec has limited vocab so has limit in determining the similarity between words, so going to use FastText(unsupervised training) to make word vectors.\n",
    "\n",
    "1. combine the preprocesed Doc object to a single text file\n",
    "2. feed that into the FastText\n",
    "3. replace the vector value of the prev. Doc as the FastText's vector\n",
    "4. do similarity tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6414\n",
      "1386\n"
     ]
    }
   ],
   "source": [
    "def is_cleanable(tk: spacy.tokens.token.Token):\n",
    "    # might(?) try and add emoji if that seems plausible\n",
    "    if (tk.is_alpha and not tk.is_stop and not tk._.is_emoji) or tk.is_sent_end:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocess_tk(tk: spacy.tokens.token.Token):\n",
    "    if not tk.is_alpha:\n",
    "        tk = '\\n'\n",
    "    elif tk.is_sent_end:\n",
    "        tk = tk.lemma_.lower().strip()\n",
    "        tk += '\\n'\n",
    "    else:\n",
    "        tk = tk.lemma_.lower().strip()\n",
    "        tk += ' '\n",
    "    \n",
    "    return tk\n",
    "\n",
    "\n",
    "# preprocessed text for FastText\n",
    "ft_preprocessed = [preprocess_tk(tk) for doc in docs for tk in doc if is_cleanable(tk)]\n",
    "print(len(ft_preprocessed))\n",
    "ft_preprocessed = ''.join(ft_preprocessed)\n",
    "ft_preprocessed = re.sub(\" +\", \" \", ft_preprocessed) # fixes double(or more) spaces\n",
    "ft_preprocessed = re.sub(\"<[^>]*>\", \"\", ft_preprocessed) # remove html tags\n",
    "\n",
    "print(ft_preprocessed.count('\\n'))\n",
    "\n",
    "ft_path = path.join(cwd, 'ft_preprocessed.txt')\n",
    "with open(ft_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(ft_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/ft_preprocessed.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  214\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   29376 lr:  0.000000 avg.loss:  4.122980 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "print(ft_path)\n",
    "model = fasttext.train_unsupervised(ft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'year',\n",
       " 'sensei',\n",
       " 'new',\n",
       " 'like',\n",
       " 'get',\n",
       " 'pull',\n",
       " 'happy',\n",
       " 'good',\n",
       " 'source',\n",
       " 'time',\n",
       " 'want',\n",
       " 'pixiv',\n",
       " 'arona',\n",
       " 'look',\n",
       " 'need',\n",
       " 'student',\n",
       " 'go',\n",
       " 'love',\n",
       " 'twitter',\n",
       " 'himari',\n",
       " 'banner',\n",
       " 'cute',\n",
       " 'hoshino',\n",
       " 'yuuka',\n",
       " 'hina',\n",
       " 'plana',\n",
       " 'chan',\n",
       " 'daughter',\n",
       " 'blue',\n",
       " 'think',\n",
       " 'ny',\n",
       " 'artist',\n",
       " 'rio',\n",
       " 'hope',\n",
       " 'mutsuki',\n",
       " 'thank',\n",
       " 'mob',\n",
       " 'give',\n",
       " 'spark',\n",
       " 'lot',\n",
       " 'let',\n",
       " 'day',\n",
       " 'bad',\n",
       " 'come',\n",
       " 'know',\n",
       " 'aru',\n",
       " 'thing',\n",
       " 'archive',\n",
       " 'end',\n",
       " 'wish',\n",
       " 'toki',\n",
       " 'correction',\n",
       " 'girl',\n",
       " 'art',\n",
       " 'see',\n",
       " 'right',\n",
       " 'well',\n",
       " 'nonomi',\n",
       " 'fuuka',\n",
       " 'spend',\n",
       " 'draw',\n",
       " 'hifumi',\n",
       " 'senseis',\n",
       " 'seagull',\n",
       " 'damn',\n",
       " 'make',\n",
       " 'bunny',\n",
       " 'kayoko',\n",
       " 'little',\n",
       " 'probably',\n",
       " 'mika',\n",
       " 'terror',\n",
       " 'read',\n",
       " 'miyu',\n",
       " 'stop',\n",
       " 'start',\n",
       " 'ready',\n",
       " 'black',\n",
       " 'fire',\n",
       " 'midori',\n",
       " 'ai',\n",
       " 'oh',\n",
       " 'money',\n",
       " 'u',\n",
       " 'nagisa',\n",
       " 'take',\n",
       " 'wait',\n",
       " 'enjoy',\n",
       " 'aris',\n",
       " 'valkyrie',\n",
       " 'delete',\n",
       " 'try',\n",
       " 'peak',\n",
       " 'feel',\n",
       " 'halo',\n",
       " 'koyuki',\n",
       " 'stay',\n",
       " 'seminar',\n",
       " 'body',\n",
       " 'game',\n",
       " 'fall',\n",
       " 'comment',\n",
       " 'big',\n",
       " 'fellow',\n",
       " 'have',\n",
       " 'translated',\n",
       " 'mean',\n",
       " 'noa',\n",
       " 'post',\n",
       " 'niko',\n",
       " 'tell',\n",
       " 'turn',\n",
       " 'suit',\n",
       " 'funny',\n",
       " 'long',\n",
       " 'will',\n",
       " 'help',\n",
       " 'squad',\n",
       " 'star',\n",
       " 'shimiko',\n",
       " 'kisaki',\n",
       " 'haruka',\n",
       " 'literally',\n",
       " 'present',\n",
       " 'fun',\n",
       " 'op',\n",
       " 'ayane',\n",
       " 'cold',\n",
       " 'swimsuit',\n",
       " 'shizuko',\n",
       " 'check',\n",
       " 'future',\n",
       " 'life',\n",
       " 'leave',\n",
       " 'wonder',\n",
       " 'rest',\n",
       " 'kivotos',\n",
       " 'srt',\n",
       " 'way',\n",
       " 'hard',\n",
       " 'typeset',\n",
       " 'beautiful',\n",
       " 'pyroxene',\n",
       " 'wife',\n",
       " 'fox',\n",
       " 'remind',\n",
       " 'image',\n",
       " 'miyako',\n",
       " 'years',\n",
       " 'r',\n",
       " 'beloved',\n",
       " 'san',\n",
       " 'kill',\n",
       " 'man',\n",
       " 'ask',\n",
       " 'bless',\n",
       " 'honestly',\n",
       " 'iori',\n",
       " 'remove',\n",
       " 'sauce',\n",
       " 'gorgeous',\n",
       " 'yandere',\n",
       " 'to',\n",
       " 'memory',\n",
       " 'say',\n",
       " 'work',\n",
       " 'soon',\n",
       " 'pretty',\n",
       " 'kaisen',\n",
       " 'care',\n",
       " 'kanna',\n",
       " 'fanart',\n",
       " 'pien',\n",
       " 'saori',\n",
       " 'community',\n",
       " 'favorite',\n",
       " 'moment',\n",
       " 'bratty',\n",
       " 'playable',\n",
       " 'pink',\n",
       " 'follow',\n",
       " 'serina',\n",
       " 'not',\n",
       " 'cuddle',\n",
       " 'strong',\n",
       " 'catch',\n",
       " 'reason',\n",
       " 'bit',\n",
       " 'friend',\n",
       " 'lobotomy',\n",
       " 'cook',\n",
       " 'fact',\n",
       " 'blessing',\n",
       " 'luck',\n",
       " 'guess',\n",
       " 'punishment',\n",
       " 'prefect',\n",
       " 'kirino',\n",
       " 'learn',\n",
       " 'rabbit',\n",
       " 'hug',\n",
       " 'precious',\n",
       " 'maid',\n",
       " 'dupe',\n",
       " 'abydos',\n",
       " 'ver',\n",
       " 'ako',\n",
       " 'ticket',\n",
       " 'flat',\n",
       " 'lol',\n",
       " 'manage',\n",
       " 'amazing',\n",
       " 'camping']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8285109996795654, 'senseis'),\n",
       " (0.55788654088974, 'funny'),\n",
       " (0.5194903016090393, '</s>'),\n",
       " (0.4845518469810486, 'archive'),\n",
       " (0.4838413894176483, 'pixiv'),\n",
       " (0.4727146625518799, 'long'),\n",
       " (0.4705008566379547, 'years'),\n",
       " (0.4613504707813263, 'ready'),\n",
       " (0.4367382824420929, 'make'),\n",
       " (0.4351038932800293, 'good')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('sensei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BAScraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
