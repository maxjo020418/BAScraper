{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from BAScraper import Pushpull\n",
    "from os import path\n",
    "\n",
    "cwd = '../results'\n",
    "data_path = path.join(cwd, 'data.json')\n",
    "\n",
    "pp = Pushpull(sleepsec=3, threads=4, cwd=cwd)\n",
    "\n",
    "data = pp.get_submissions(after=datetime(2024, 1, 1), before=datetime(2024, 1, 2),\n",
    "                          subreddit='bluearchive', get_comments=True, duplicate_action='keep_original')\n",
    "\n",
    "with open(data_path, \"w\", encoding='utf-8') as outfile:\n",
    "    json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "from os import path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "cwd = '../results'\n",
    "data_path = path.join(cwd, 'data.json')\n",
    "student_path = path.join(cwd, 'students.txt')\n",
    "\n",
    "exclude_author = ['AutoModerator', 'BlueArchive-ModTeam']\n",
    "\n",
    "with open(data_path, \"r\", encoding='utf-8') as outfile:\n",
    "    data = json.load(outfile)\n",
    "\n",
    "data_df = pandas.DataFrame.from_dict(data, orient='index')\n",
    "# [print(col) for col in data_df.columns]\n",
    "filtered_df = data_df[['title', 'link_flair_text', 'ups', 'created_utc', 'comments', 'num_comments']]\n",
    "filtered_df.created_utc = filtered_df.created_utc.apply(lambda x: datetime.fromtimestamp(x))\n",
    "filtered_df.comments = filtered_df.comments.apply(lambda x: [comment['body'] for comment in x if comment['author'] not in exclude_author])\n",
    "filtered_df.num_comments = filtered_df.comments.apply(lambda x: len(x))\n",
    "filtered_df = filtered_df.loc[filtered_df['link_flair_text'].isin(['OC ART ', 'NON OC ART ', 'Comic/TL'])]\n",
    "\n",
    "display(filtered_df.nlargest(10, 'num_comments'))\n",
    "Counter(filtered_df.link_flair_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the title for the post is appended to the start of the comment doc group**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "sub_emote_pattern = r\"!\\[img\\]\\(emote\\|t5_[a-z0-9]+\\|\\d+\\)\"\n",
    "url_pattern = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
    "\n",
    "patterns = [sub_emote_pattern, url_pattern]\n",
    "\n",
    "def clean(text):\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "        text.replace('senseis', 'senseis') # problems with the senseis plural in the model\n",
    "    return html.unescape(text).lower()\n",
    "\n",
    "data_text = { i: [v.title] + v.comments for i, v in filtered_df.iterrows()}\n",
    "data_text = { i: clean('\\n'.join(post_group)) for i, post_group in data_text.items()}\n",
    "\n",
    "data_text['18vokb1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacymoji import Emoji\n",
    "\n",
    "print(spacy.require_gpu())\n",
    "\n",
    "student_list = list()\n",
    "with open(student_path, 'r', encoding='utf-8') as f:\n",
    "    for l in f.readlines():\n",
    "        student_list.append(l.strip())\n",
    "\n",
    "# alias groups - (main name, alias-1, alias-2, alias-3, ...)\n",
    "alias_groups = [\n",
    "    ('aris', 'alice', 'arisu', 'fridge', 'refrigerator'),\n",
    "    ('hoshino', 'oji-san', 'ojisan', 'oji san'),\n",
    "    ('yuuka', 'yuka', 'calculator', '100kg', '100 kg'),\n",
    "    # ('sensei', 'senseis'), # cannot be detected as plural\n",
    "]\n",
    "\n",
    "student_list.extend([j for i in alias_groups for j in i])\n",
    "\n",
    "print('student_list:', len(student_list))\n",
    "print(student_list)\n",
    "\n",
    "\n",
    "def BA_char_finder(tk):\n",
    "    if (student := tk.text.lower()) in student_list:\n",
    "        for group in alias_groups:\n",
    "            if student in group:\n",
    "                student = group[0]\n",
    "        return student\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_nbors_(tk: spacy.tokens.token.Token, step: int, N: int):\n",
    "    # step usually should be 1 or -1 - searching range step\n",
    "    # N is the Nth search result to be returned\n",
    "\n",
    "    if tk.is_sent_start and step < 0:\n",
    "        return\n",
    "    elif tk.is_sent_end and step > 0:\n",
    "        return\n",
    "    \n",
    "    n = 1\n",
    "    i = step\n",
    "    while True:\n",
    "        nbor = tk.nbor(i)\n",
    "        if nbor.is_stop or nbor._.is_emoji or not nbor.is_alpha:\n",
    "            pass\n",
    "        else:\n",
    "            if n == N:\n",
    "                return nbor\n",
    "            else:\n",
    "                n += 1\n",
    "                pass\n",
    "        \n",
    "        if nbor.is_sent_end or nbor.is_sent_start:\n",
    "            return\n",
    "\n",
    "        i += step\n",
    "\n",
    "\n",
    "def get_nbors(tk):\n",
    "    return ((\n",
    "        (\n",
    "        get_nbors_(tk, -1, 1), # l1\n",
    "        get_nbors_(tk, 1, 1), # r1\n",
    "        ),\n",
    "        (\n",
    "        get_nbors_(tk, -1, 2), # l2\n",
    "        get_nbors_(tk, 1, 2), # r2\n",
    "        ),\n",
    "        # tk.sent\n",
    "    ))\n",
    "\n",
    "# for splitting sentences\n",
    "@spacy.language.Language.component(\"custom_boundary\")\n",
    "def custom_boundary(doc):\n",
    "\n",
    "    delimiters=['...', '\\n']\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        if token.text in delimiters:\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "nlp.add_pipe('custom_boundary', before='parser')\n",
    "\n",
    "Token.set_extension(\"nbors\", getter=get_nbors, force=True)\n",
    "Token.set_extension(\"ba_characters\", getter=BA_char_finder, force=True)\n",
    "\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### planned pipeline\n",
    "\n",
    "first, use the `set_extention` to set all the student attr. might add it as a custom pipeline. since the students have alternate names, nicknames I should consider that\n",
    "\n",
    "**for getting co-occurrence matrix for students**\n",
    "1. iter through the tokens\n",
    "2. if `student` extension returns a student: start the below, else: continue\n",
    "3. get the neighboring words (maybe a range of 2 on each side, and give the closer one higher score)\n",
    "4. if the neighbor token: is a stopword(`is_stop`): pass, is an emoji(`is_emoji`): pass\n",
    "5. expand the search range until the required number of token(2 on each side) is found\n",
    "6. lowercase (and probably clean up a bit) the token and add to list with required metadata(such as score)\n",
    "7. score for each descriptive token is determined using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuffs to use\n",
    "\n",
    "**token**\n",
    "- get/set_extension\n",
    "- nbor\n",
    "- similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(data_text['18vnj59'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tk in enumerate(doc):\n",
    "    print(f'{tk.lemma_} \\t{tk.is_sent_start} / {tk.is_sent_end} - <{i}>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([(emote[0], emote[2]) for emote in doc._.emoji]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([tk._.ba_characters for tk in doc if tk._.ba_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tk._.nbors for tk in doc if tk._.ba_characters == 'sensei']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scoring (default embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del doc\n",
    "except: pass\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "docs = list()\n",
    "for t in tqdm(data_text.values()):\n",
    "    docs.append(nlp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([tk._.ba_characters for doc in docs for tk in doc if tk._.ba_characters]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_character = 'sensei'\n",
    "\n",
    "# `cm` as in correlation matrix (not exactly a matrix but...)\n",
    "\n",
    "cm_original = [tk._.nbors for doc in docs for tk in doc if tk._.ba_characters == target_character]\n",
    "cm_original_1 = [j for i in cm_original for j in i[0] if j] # bigram range\n",
    "cm_original_2 = [j for i in cm_original for j in i[1] if j] # trigram range\n",
    "\n",
    "print(Counter([i.text for i in cm_original_1]).most_common(10))\n",
    "print(Counter([i.text for i in cm_original_2]).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hard to do vector similarity because `en_core_web_lg` doesn't have vector values for some of the BA_characters**\n",
    "\n",
    "also, not sure if the vector values will actually represent the character\n",
    "\n",
    "to note:\n",
    ">SpaCy's native models, such as en_core_web_lg, the position of a word in a sentence does not affect its vector representation. These models use static word embeddings, where each word is assigned a fixed vector based on the word itself, regardless of its position or the context in which it appears.\n",
    "\n",
    "so it's okay to de-dupluicate all the words with the same `Token.text` value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [tk for doc in docs for tk in doc if tk._.ba_characters == target_character]\n",
    "print(Counter([t.text for t in targets]))\n",
    "\n",
    "# remove duplicate tokens for targets\n",
    "dedup = {\n",
    "    target.text: target for target in targets \n",
    "}\n",
    "targets = [t for t in dedup.values()]\n",
    "\n",
    "for t in targets:\n",
    "    assert t.has_vector, f\"{t.text} doesn't have a vector value\"\n",
    "\n",
    "print('`cm_original` no vector:')\n",
    "[print(t) for t in cm_original_1 if not t.has_vector]\n",
    "\n",
    "vec_sim = {target.text: {tk.text: tk.similarity(target) for tk in cm_original_1 if tk.has_vector} for target in tqdm(targets) if target.has_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([k for k in vec_sim])\n",
    "print(len(vec_sim))\n",
    "{k: v for k, v in sorted(vec_sim['sensei'].items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FastText to deal with OOV tokens\n",
    "above scoring and text2vec has limited vocab so has limit in determining the similarity between words, so going to use FastText(unsupervised training) to make word vectors.\n",
    "\n",
    "1. combine the preprocesed Doc object to a single text file\n",
    "2. feed that into the FastText\n",
    "3. replace the vector value of the prev. Doc as the FastText's vector\n",
    "4. do similarity tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cleanable(tk: spacy.tokens.token.Token):\n",
    "    # might(?) try and add emoji if that seems plausible\n",
    "    if (tk.is_alpha and not tk.is_stop and not tk._.is_emoji) or tk.is_sent_end:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocess_tk(tk: spacy.tokens.token.Token):\n",
    "    if not tk.is_alpha:\n",
    "        tk = '\\n'\n",
    "    elif tk.is_sent_end:\n",
    "        tk = tk.lemma_.lower().strip()\n",
    "        tk += '\\n'\n",
    "    else:\n",
    "        tk = tk.lemma_.lower().strip()\n",
    "        tk += ' '\n",
    "    \n",
    "    return tk\n",
    "\n",
    "\n",
    "# preprocessed text for FastText\n",
    "ft_preprocessed = [preprocess_tk(tk) for doc in docs for tk in doc if is_cleanable(tk)]\n",
    "print(len(ft_preprocessed))\n",
    "ft_preprocessed = ''.join(ft_preprocessed)\n",
    "ft_preprocessed = re.sub(\" +\", \" \", ft_preprocessed) # fixes double(or more) spaces\n",
    "ft_preprocessed = re.sub(\"<[^>]*>\", \"\", ft_preprocessed) # remove html tags\n",
    "\n",
    "print(ft_preprocessed.count('\\n'))\n",
    "\n",
    "ft_path = path.join(cwd, 'ft_preprocessed.txt')\n",
    "with open(ft_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(ft_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "print(ft_path)\n",
    "model = fasttext.train_unsupervised(ft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_nearest_neighbors('sensei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BAScraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
